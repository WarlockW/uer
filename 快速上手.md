## 使用BERT模型预训练和分类
这里我们通过常用的例子来简要说明如何使用UER-py，更多的细节请参考使用说明章节。我们首先使用BERT模型和[豆瓣书评分类数据集](https://embedding.github.io/evaluation/)。我们在书评语料上对模型进行预训练，然后在书评分类数据集上对其进行微调。这个过程有三个输入文件：书评语料，书评分类数据集和中文词典。这些文件均为UTF-8编码，并被包括在这个项目中。

BERT模型要求的预训练语料格式是一行一个句子，不同文档使用空行分隔，如下所示：

```
doc1-sent1
doc1-sent2
doc1-sent3

doc2-sent1

doc3-sent1
doc3-sent2
```
书评语料是由书评分类数据集去掉标签得到的。我们将一条评论从中间分开，从而形成一个两句话的文档，具体可见*corpora*文件夹中的*book_review_bert.txt*。

分类数据集的格式如下：
```
label    text_a
1        instance1
0        instance2
1        instance3
```
标签和文本之间用\t分隔，第一行是列名。对于n分类，标签应该是0到n-1之间（包括0和n-1）的整数。

词典文件的格式是一行一个单词，我们使用谷歌提供的包含21128个中文字符的词典文件*models/google_zh_vocab.txt*

我们首先对书评语料进行预处理。预处理阶段需要指定模型的目标任务（*--target*）：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target bert
```
注意我们需要安装 ``six>=1.12.0``。

预处理非常耗时，使用多个进程可以大大加快预处理速度（*--processes_num*）。默认的分词器为 *--tokenizer bert* 。原始文本在预处理之后被转换为*pretrain.py*的可以接受的输入，*dataset.pt*。然后下载Google中文预训练模型[*google_zh_model.bin*](https://share.weiyun.com/A1C49VPb)（此文件为UER支持的格式，原始模型来自于[这里](https://github.com/google-research/bert)），并将其放在 *models* 文件夹中。接着加载Google中文预训练模型，在书评语料上对其进行增量预训练。预训练模型由词向量层，编码层和目标任务层组成。因此要构建预训练模型，我们应明确指定模型的词向量层（*--embedding*），编码器层（*--encoder* 和 *--mask*）和目标任务层（*--target*）的类型。假设我们有一台带有8个GPU的机器：
```
python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --pretrained_model_path models/google_zh_model.bin \
                    --output_model_path models/book_review_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 5000 --save_checkpoint_steps 1000 --batch_size 32 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target bert

mv models/book_review_model.bin-5000 models/book_review_model.bin
```
*--mask* 指定注意力网络中使用的遮罩类型。BERT使用双向语言模型，句子中的任意一个词可以看到所有词的信息，因此我们使用 *fully_visible* 遮罩类型。BERT模型的词向量层是word（token）、position、segment向量的求和，因此我们使用 *--embedding word_pos_seg* 。默认情况下，配置文件为 *models/bert/base_config.json* 。配置文件指定了模型的超参数。
请注意，*pretrain.py*输出的模型会带有记录训练步数的后缀（*--total_steps*），这里我们可以删除后缀以方便使用。

然后，我们在下游分类数据集上微调预训练模型，我们使用 *pretrain.py* 的输出[*book_review_model.bin*](https://share.weiyun.com/xOFsYxZA)：
```
python3 run_classifier.py --pretrained_model_path models/book_review_model.bin \
                          --vocab_path models/google_zh_vocab.txt \
                          --train_path datasets/douban_book_review/train.tsv \
                          --dev_path datasets/douban_book_review/dev.tsv \
                          --test_path datasets/douban_book_review/test.tsv \
                          --epochs_num 3 --batch_size 32 \
                          --embedding word_pos_seg --encoder transformer --mask fully_visible
``` 
*book_review_model.bin* 在书评分类任务测试集上的准确率为88.2。值得注意的是，我们不需要在微调阶段指定目标任务。预训练模型的目标任务已被替换为特定下游任务需要的目标任务。

微调后的模型的默认路径是*models/finetuned_model.bin*, 然后我们利用微调后的分类器模型进行预测：
```
python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv \
                                          --labels_num 2 \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```
*--test_path* 指定需要预测的文件，文件需要包括text_a列；<br>
*--prediction_path* 指定预测结果的文件；<br>
注意到我们需要指定分类任务标签的个数 *--labels_num* ，这里是二分类任务。

我们还可以使用 *google_zh_model.bin* 在下游分类数据集上微调：
```
python3 run_classifier.py --pretrained_model_path models/google_zh_model.bin \
                          --vocab_path models/google_zh_vocab.txt \
                          --train_path datasets/douban_book_review/train.tsv \
                          --dev_path datasets/douban_book_review/dev.tsv \
                          --test_path datasets/douban_book_review/test.tsv \
                          --epochs_num 3 --batch_size 32 \
                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```
谷歌模型的准确率为87.5。

<br>

## 指定使用的GPU
推荐使用CUDA_VISIBLE_DEVICES指定程序可见的GPU（如果不指定，则使用所有的GPU）。假设我们需要使用0号GPU和2号GPU：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                      --processes_num 8 --target bert

CUDA_VISIBLE_DEVICES=0,2 python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --pretrained_model_path models/google_zh_model.bin \
                                             --output_model_path models/book_review_model.bin  --world_size 2 --gpu_ranks 0 1 \
                                             --total_steps 5000 --save_checkpoint_steps 1000 --batch_size 32 --embedding word_pos_seg --encoder transformer --mask fully_visible --target bert

mv models/book_review_model.bin-5000 models/book_review_model.bin

CUDA_VISIBLE_DEVICES=0,2 python3 run_classifier.py --pretrained_model_path models/book_review_model.bin --vocab_path models/google_zh_vocab.txt \
                                                   --train_path datasets/douban_book_review/train.tsv --dev_path datasets/douban_book_review/dev.tsv --test_path datasets/douban_book_review/test.tsv \
                                                   --output_model_path models/classifier_model.bin \
                                                   --epochs_num 3 --batch_size 32 --embedding word_pos_seg --encoder transformer --mask fully_visible

CUDA_VISIBLE_DEVICES=0,2 python3 inference/run_classifier_infer.py --load_model_path models/classifier_model.bin --vocab_path models/google_zh_vocab.txt \
                                                                   --test_path datasets/douban_book_review/test_nolabel.tsv \
                                                                   --prediction_path datasets/douban_book_review/prediction.tsv --labels_num 2 \
                                                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
注意到我们在微调阶段使用 *--output_model_path* 指定微调后的模型的输出路径。预训练的实际batch size大小是 *--batch_size* 乘以 *--world_size*；分类的实际的batch size大小是 *--batch_size* 。

<br>

## 使用MLM目标任务预训练
预测是否是下一个句子（NSP）是BERT的目标任务之一，但是，NSP任务不适合句子级别的评论，因为我们需要将句子切分为多个部分去构造文档。 UER-py可以让用户自由选择不同的目标任务。这里我们选择使用遮罩语言模型（MLM）作为目标任务。MLM目标任务对书籍评论语料可能是更合适的选择：

```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                      --processes_num 8 --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --pretrained_model_path models/google_zh_model.bin \
                    --output_model_path models/book_review_mlm_model.bin  --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 5000 --save_checkpoint_steps 2500 --batch_size 32 --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm

mv models/book_review_mlm_model.bin-5000 models/book_review_mlm_model.bin

CUDA_VISIBLE_DEVICES=0,1 python3 run_classifier.py --pretrained_model_path models/book_review_mlm_model.bin --vocab_path models/google_zh_vocab.txt \
                                                   --train_path datasets/douban_book_review/train.tsv --dev_path datasets/douban_book_review/dev.tsv --test_path datasets/douban_book_review/test.tsv \
                                                   --epochs_num 3 --batch_size 32 --embedding word_pos_seg --encoder transformer --mask fully_visible
```
不同的预训练目标需要不同格式的语料。MLM目标对应的语料格式为一行一个文档：
```
doc1
doc2
doc3
``` 
注意到当预训练目标被改为MLM后，我们使用的预训练语料为 *corpora/book_review.txt* 而不是 *corpora/book_review_bert.txt*。

<br>

## 使用Transformer之外的编码器
BERT参数量大，计算较慢。我们希望加速模型的同时让模型仍然在下游任务上有好的表现。这里我们选择2层LSTM编码器来替代12层Transformer编码器。我们首先下载2层LSTM编码器的预训练模型[*cluecorpussmall_lstm_lm_model.bin*](https://share.weiyun.com/XFc4hcn6)。这个预训练模型在[CLUECorpusSmall](https://github.com/CLUEbenchmark/CLUECorpus2020)语料上训练了50万步：
```
python3 preprocess.py --corpus_path corpora/cluecorpussmall.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                     --processes_num 8 --seq_length 256 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/cluecorpussmall_lstm_lm_model.bin \
                    --config_path models/rnn_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 500000 --save_checkpoint_steps 100000 \
                    --learning_rate 1e-3 --batch_size 64 \
                    --embedding word --remove_embedding_layernorm --encoder lstm --target lm
```
把预训练模型记录训练步数的后缀去掉，然后在下游分类数据集上对其进行微调：
```
python3 run_classifier.py --pretrained_model_path models/cluecorpussmall_lstm_lm_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/rnn_config.json \
                          --train_path datasets/douban_book_review/train.tsv --dev_path datasets/douban_book_review/dev.tsv --test_path datasets/douban_book_review/test.tsv \
                          --learning_rate 1e-3 --batch_size 64 --epochs_num 5 \
                          --embedding word --remove_embedding_layernorm --encoder lstm --pooling mean

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/rnn_config.json --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv \
                                          --labels_num 2 --embedding word --remove_embedding_layernorm --encoder lstm --pooling mean
```
我们可以在书评分类任务测试集上得到84.6的准确率。相比之下，我们使用相同的LSTM编码器，但是不加载预训练模型，只能得到约81的准确率。
<br>

UER-py还支持更多的预训练模型。 <br>
我们下载ELMo预训练模型[*cluecorpussmall_elmo_model.bin*](https://share.weiyun.com/xezGTd86)。这个预训练模型在CLUECorpusSmall语料上训练了50万步：
```
python3 preprocess.py --corpus_path corpora/cluecorpussmall.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                     --processes_num 8 --seq_length 256 --target bilm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/cluecorpussmall_elmo_model.bin \
                    --config_path models/birnn_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 500000 --save_checkpoint_steps 100000 \
                    --learning_rate 5e-4 --batch_size 64 \
                    --embedding word --remove_embedding_layernorm --encoder bilstm --target bilm
```
把预训练模型记录训练步数的后缀去掉，然后在Chnsenticorp情感分类数据集上对其进行增量预训练和微调：
```
python3 preprocess.py --corpus_path corpora/chnsenticorp.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                      --processes_num 8 --seq_length 192 --target bilm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --pretrained_model_path models/cluecorpussmall_elmo_model.bin \
                    --config_path models/birnn_config.json \
                    --output_model_path models/chnsenticorp_elmo_model.bin --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 5000 --save_checkpoint_steps 2500 --batch_size 64 --learning_rate 5e-4 \
                    --embedding word --remove_embedding_layernorm --encoder bilstm --target bilm

mv models/chnsenticorp_elmo_model.bin-5000 models/chnsenticorp_elmo_model.bin

python3 run_classifier.py --pretrained_model_path models/chnsenticorp_elmo_model.bin --vocab_path models/google_zh_vocab.txt --config_path models/birnn_config.json \
                          --train_path datasets/chnsenticorp/train.tsv --dev_path datasets/chnsenticorp/dev.tsv --test_path datasets/chnsenticorp/test.tsv \
                          --epochs_num 5  --batch_size 64 --seq_length 192 --learning_rate 5e-4 \
                          --embedding word --remove_embedding_layernorm --encoder bilstm --pooling max

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/birnn_config.json \
                                          --test_path datasets/chnsenticorp/test_nolabel.tsv \
                                          --prediction_path datasets/chnsenticorp/prediction.tsv \
                                          --labels_num 2 --embedding word --remove_embedding_layernorm --encoder bilstm --pooling max
```
*corpora/chnsenticorp.txt* 是由Chnsenticorp数据集去掉标签得到的。

在Chnsenticorp数据集上微调GatedCNN模型的示例：
```
python3 run_classifier.py --pretrained_model_path models/cluecorpussmall_gatedcnn_lm_model.bin \
                          --vocab_path models/google_zh_vocab.txt \
                          --config_path models/gatedcnn_9_config.json \
                          --train_path datasets/chnsenticorp/train.tsv --dev_path datasets/chnsenticorp/dev.tsv --test_path datasets/chnsenticorp/test.tsv \
                          --epochs_num 5  --batch_size 64 --learning_rate 5e-5 \
                          --embedding word --remove_embedding_layernorm --encoder gatedcnn --pooling mean

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/gatedcnn_9_config.json \
                                          --test_path datasets/chnsenticorp/test_nolabel.tsv \
                                          --prediction_path datasets/chnsenticorp/prediction.tsv \
                                          --labels_num 2 --embedding word --remove_embedding_layernorm --encoder gatedcnn --pooling mean
```
用户可以从[这里](https://share.weiyun.com/VLe8O6kM)下载 *wikizh_gatedcnn_lm_model.bin*。这个预训练模型在CLUECorpusSmall语料上训练了50万步：
```
python3 preprocess.py --corpus_path corpora/cluecorpussmall.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                     --processes_num 8 --seq_length 256 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --config_path models/gatedcnn_9_config.json \
                    --output_model_path models/cluecorpussmall_gatedcnn_lm_model.bin --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 500000 --save_checkpoint_steps 100000 --report_steps 100 --learning_rate 1e-4 --batch_size 64 \
                    --embedding word --remove_embedding_layernorm --encoder gatedcnn --target lm
```
<br>

## 分类任务交叉验证
UER-py支持分类任务的交叉验证，在竞赛数据集[SMP2020-EWECT](http://39.97.118.137/)上使用交叉验证的示例：
```
CUDA_VISIBLE_DEVICES=0 python3 run_classifier_cv.py --pretrained_model_path models/google_zh_model.bin \
                                                    --vocab_path models/google_zh_vocab.txt \
                                                    --config_path models/bert/base_config.json \
                                                    --output_model_path models/classifier_model.bin \
                                                    --train_features_path datasets/smp2020-ewect/virus/train_features.npy \
                                                    --train_path datasets/smp2020-ewect/virus/train.tsv \
                                                    --epochs_num 3 --batch_size 32 --folds_num 5 \
                                                    --embedding word_pos_seg --encoder transformer --mask fully_visible
```
*google_zh_model.bin* 的结果为79.1/63.8（准确率/F1值）；<br>
*--folds_num* 指定交叉验证的轮数；<br>
*--output_path* 指定微调模型的路径，共保存 *--folds_num* 个微调后的模型，并将 *fold ID* 后缀添加到模型名称中；<br>
*--train_features_path* 指定out-of-fold预测文件的路径；训练集被分成了 *--folds_num* 折。一折样本的预测概率是由其他折上的数据训练的模型预测得到的。*train_features.npy* 可用于stacking集成。[竞赛解决方案](https://github.com/dbiir/UER-py/wiki/竞赛解决方案)部分给出了更多详细信息。<br>

我们可以进一步尝试不同的预训练模型。例如，可以下载[*RoBERTa-wwm-ext-large from HIT*](https://github.com/ymcui/Chinese-BERT-wwm)并将其转换为UER格式：
```
python3 scripts/convert_bert_from_huggingface_to_uer.py --input_model_path models/chinese_roberta_wwm_large_ext_pytorch/pytorch_model.bin \
                                                        --output_model_path models/chinese_roberta_wwm_large_ext_pytorch/pytorch_model_uer.bin \
                                                        --layers_num 24

CUDA_VISIBLE_DEVICES=0,1 python3 run_classifier_cv.py --pretrained_model_path models/chinese_roberta_wwm_large_ext_pytorch/pytorch_model_uer.bin \
                                                      --vocab_path models/google_zh_vocab.txt \
                                                      --config_path models/bert/large_config.json \
                                                      --train_path datasets/smp2020-ewect/virus/train.tsv \
                                                      --train_features_path datasets/smp2020-ewect/virus/train_features.npy \
                                                      --epochs_num 3 --batch_size 64 --folds_num 5 \
                                                      --embedding word_pos_seg --encoder transformer --mask fully_visible
```
*RoBERTa-wwm-ext-large* 的结果为80.3/66.8（准确率/F1值）。 <br>
使用我们的预训练模型[*Reviews+BertEncoder(large)+MlmTarget*](https://share.weiyun.com/hn7kp9bs)的示例如下（更多详细信息，请参见模型仓库）：
```
CUDA_VISIBLE_DEVICES=0,1 python3 run_classifier_cv.py --pretrained_model_path models/reviews_bert_large_mlm_model.bin \
                                                      --vocab_path models/google_zh_vocab.txt \
                                                      --config_path models/bert/large_config.json \
                                                      --train_path datasets/smp2020-ewect/virus/train.tsv \
                                                      --train_features_path datasets/smp2020-ewect/virus/train_features.npy \
                                                      --folds_num 5 --epochs_num 3 --batch_size 64 --seed 17 \
                                                      --embedding word_pos_seg --encoder transformer --mask fully_visible
```
结果为81.3/68.4（准确率/F1值），与其他开源预训练模型相比，这一结果很具有竞争力。这个预训练模型使用的语料和SMP2020-EWECT（微博评论数据集）高度相似。 <br>
有时大模型无法收敛，我们需要通过指定 *--seed* 尝试不同的随机种子。

<br>

## 使用BERT模型微调下游任务
除了分类外，UER-py还可以用于其他多种下游任务的微调。例如，*run_classifier.py*还可以用于文本对分类任务。在下游任务数据集章节中，我们可以下载文本对分类数据集LCQMC并在其之上进行微调：
```
python3 run_classifier.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                          --train_path datasets/lcqmc/train.tsv --dev_path datasets/lcqmc/dev.tsv --test_path datasets/lcqmc/test.tsv \
                          --output_model_path models/classifier_model.bin \
                          --batch_size 32 --epochs_num 3 --seq_length 128 \
                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```
对于文本对分类，数据集需要包括text_a、text_b、label列。

然后我们使用文本对分类模型进行推理：
```
python3 inference/run_classifier_infer.py --load_model_path models/classifier_model.bin --vocab_path models/google_zh_vocab.txt \
                                          --test_path datasets/lcqmc/test.tsv \
                                          --prediction_path datasets/lcqmc/prediction.tsv --labels_num 2 --seq_length 128 \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```
被预测的文件（*--test_path*）需要包括text_a和text_b列。
<br>

我们可以使用*run_ner.py*进行命名实体识别：
```
python3 run_ner.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                   --train_path datasets/msra_ner/train.tsv --dev_path datasets/msra_ner/dev.tsv --test_path datasets/msra_ner/test.tsv \
                   --output_model_path models/ner_model.bin \
                   --label2id_path datasets/msra_ner/label2id.json --epochs_num 5 --batch_size 16 \
                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
*--label2id_path* 指定用于命名实体识别的label2id文件的路径。

然后我们使用ner模型进行推理：
```
python3 inference/run_ner_infer.py --load_model_path models/ner_model.bin --vocab_path models/google_zh_vocab.txt \
                                   --test_path datasets/msra_ner/test_nolabel.tsv \
                                   --prediction_path datasets/msra_ner/prediction.tsv \
                                   --label2id_path datasets/msra_ner/label2id.json \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
<br>

我们可以使用 *run_cmrc.py* 进行机器阅读理解：
```
python3 run_cmrc.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                    --train_path datasets/cmrc2018/train.json --dev_path datasets/cmrc2018/dev.json \
                    --output_model_path models/cmrc_model.bin \
                    --epochs_num 2 --batch_size 8 --seq_length 512 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible
```
我们不指定 *--test_path*，因为CMRC2018数据集不提供测试集的标签。
然后，我们使用cmrc模型进行推理：
```
python3 inference/run_cmrc_infer.py --load_model_path models/cmrc_model.bin --vocab_path models/google_zh_vocab.txt \
                                    --test_path datasets/cmrc2018/test.json  \
                                    --prediction_path datasets/cmrc2018/prediction.json --seq_length 512 \
                                    --embedding word_pos_seg --encoder transformer --mask fully_visible
```

<br>

## 使用语言模型微调和生成
使用GPT-2模型微调下游任务示例：
```
python3 run_classifier.py --pretrained_model_path models/cluecorpussmall_gpt2_seq1024_model.bin --vocab_path models/google_zh_vocab.txt \
                          --config_path models/gpt2/config.json \
                          --train_path datasets/douban_book_review/train.tsv --dev_path datasets/douban_book_review/dev.tsv --test_path datasets/douban_book_review/test.tsv \
                          --epochs_num 3 --batch_size 32 \
                          --embedding word_pos --remove_embedding_layernorm \
                          --encoder transformer --mask causal --layernorm_positioning pre --pooling mean
```
使用GPT-2模型生成文本示例：
```
python3 scripts/generate_lm.py --load_model_path models/cluecorpussmall_gpt2_seq1024_model.bin-250000 \
                               --vocab_path models/google_zh_vocab.txt \
                               --test_path beginning.txt --prediction_path generated_text.txt \
                               --config_path models/gpt2/config.json --seq_length 128 \
                               --embedding word_pos --remove_embedding_layernorm \
                               --encoder transformer --mask causal --layernorm_positioning pre \
                               --target lm --tie_weights
```
用户可以从[这里](https://share.weiyun.com/0eAlQWRB)下载 *cluecorpussmall_gpt2_seq1024_model.bin* 。

使用[LSTM语言模型](https://share.weiyun.com/XFc4hcn6)生成文本示例：
```
python3 scripts/generate_lm.py --load_model_path models/cluecorpussmall_lstm_lm_model.bin --vocab_path models/google_zh_vocab.txt \
                               --test_path beginning.txt --prediction_path generated_text.txt \
                               --config_path models/rnn_config.json --seq_length 128 \
                               --embedding word --remove_embedding_layernorm \
                               --encoder lstm --target lm
```

使用[GatedCNN语言模型](https://share.weiyun.com/VLe8O6kM)生成文本示例：
```
python3 scripts/generate_lm.py --load_model_path models/cluecorpussmall_gatedcnn_lm_model.bin --vocab_path models/google_zh_vocab.txt \
                               --test_path beginning.txt --prediction_path generated_text.txt \
                               --config_path models/gatedcnn_9_config.json --seq_length 128 \
                               --embedding word --remove_embedding_layernorm \
                               --encoder gatedcnn --target lm
```

<br>

## 使用不同的分词器和词典
在大多数情况下，我们使用 *--vocab_path models/google_zh_vocab.txt* 和 *--tokenizer bert* 组合进行分词。在项目的大多数脚本中， *--tokenizer bert* 作为默认的分词器被使用。因此我通常不会显式的指定 *--tokenizer* 。这里我们展示更多的分词器和词典的使用方法。

*--tokenizer bert* 在处理中文的时候是基于字的。如果我们想得到基于词的预训练模型并基于其进行微调和推理，那么首先我们要对语料进行分词，词与词之间用空格分隔。然后，我们基于分词后的语料（*corpora/book_review_seg.txt*）构建词典：
```
python3 scripts/build_vocab.py --corpus_path corpora/book_review_seg.txt \
                               --vocab_path models/book_review_word_vocab.txt \
                               --tokenizer space --workers_num 8 --min_count 5 
```
由于词与词之间用空格分隔，因此在预处理和预训练阶段需要使用 *--tokenizer space* 。基于词的预训练和预处理示例：
```
python3 preprocess.py --corpus_path corpora/book_review_seg.txt \
                      --vocab_path models/book_review_word_vocab.txt  --tokenizer space \
                      --dataset_path book_review_word_dataset.pt \
                      --processes_num 8 --seq_length 128 --dynamic_masking --target mlm

python3 pretrain.py --dataset_path book_review_word_dataset.pt \
                    --vocab_path models/book_review_word_vocab.txt  --tokenizer space \
                    --output_model_path models/book_review_word_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 5000 --save_checkpoint_steps 2500 --report_steps 500 \
                    --learning_rate 1e-4 --batch_size 64 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm --tie_weights
```
在下游任务微调和推理的过程中，我们同样需要显式的指定 *--vocab_path models/book_review_word_vocab.txt* 和 *--tokenizer space* ，并且数据集的训练/验证/测试集文本列（text_a和text_b）需要使用相同的分词工具进行分词。对 *datasets/douban_book_review/* 中的文件进行分词得到 *datasets/douban_book_review_seg/* ：
```
mv models/book_review_word_model.bin-5000 models/book_review_word_model.bin

python3 run_classifier.py --pretrained_model_path models/book_review_word_model.bin \
                          --vocab_path models/book_review_word_vocab.txt  --tokenizer space \
                          --train_path datasets/douban_book_review_seg/train.tsv --dev_path datasets/douban_book_review_seg/dev.tsv --test_path datasets/douban_book_review_seg/test.tsv \
                          --epochs_num 3 --batch_size 32 --embedding word_pos_seg --encoder transformer --mask fully_visible

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/book_review_word_vocab.txt  --tokenizer space \
                                          --test_path datasets/douban_book_review_seg/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review_seg/prediction.tsv --labels_num 2 \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```

使用SentencePiece分词示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt \
                      --spm_model_path models/cluecorpussmall_spm.model \
                      --dataset_path book_review_word_sp_dataset.pt \
                      --processes_num 8 --seq_length 128 --dynamic_masking --target mlm

python3 pretrain.py --dataset_path book_review_word_sp_dataset.pt \
                    --spm_model_path models/cluecorpussmall_spm.model \
                    --output_model_path models/book_review_word_sp_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 5000 --save_checkpoint_steps 2500 --report_steps 500 \
                    --learning_rate 1e-4 --batch_size 64 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm --tie_weights

mv models/book_review_word_sp_model.bin-5000 models/book_review_word_sp_model.bin

python3 run_classifier.py --pretrained_model_path models/book_review_word_sp_model.bin \
                          --spm_model_path models/cluecorpussmall_spm.model \
                          --train_path datasets/douban_book_review/train.tsv --dev_path datasets/douban_book_review/dev.tsv --test_path datasets/douban_book_review/test.tsv \
                          --epochs_num 3 --batch_size 32 --embedding word_pos_seg --encoder transformer --mask fully_visible

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --spm_model_path models/cluecorpussmall_spm.model \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv --labels_num 2 \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```

如果使用基于字的分词方式，可以使用 *--vocab_path models/google_zh_vocab.txt* 和 *--tokenizer char* 的组合代替上面的 *--spm_model_path models/cluecorpussmall_spm.model*，其余的选项不变。
由于谷歌提供的词典 *models/google_zh_vocab.txt* 是基于字的，因此可以直接使用。

更深入的细节原理请参见[分词和词典](https://github.com/dbiir/UER-py/wiki/分词和词典)
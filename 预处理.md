```
usage: preprocess.py [-h] --corpus_path CORPUS_PATH [--vocab_path VOCAB_PATH]
                     [--spm_model_path SPM_MODEL_PATH]
                     [--dataset_path DATASET_PATH]
                     [--tokenizer {bert,char,space}]
                     [--processes_num PROCESSES_NUM]
                     [--target {bert,lm,cls,mlm,bilm,albert}]
                     [--docs_buffer_size DOCS_BUFFER_SIZE]
                     [--seq_length SEQ_LENGTH] [--dup_factor DUP_FACTOR]
                     [--short_seq_prob SHORT_SEQ_PROB] [--full_sentences]
                     [--seed SEED] [--dynamic_masking] [--span_masking]
                     [--span_geo_prob SPAN_GEO_PROB]
                     [--span_max_length SPAN_MAX_LENGTH]
```
用户必须在进行预训练之前对语料进行预处理。 <br>
在单台机器上进行预处理的示例：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                      --processes_num 8 --target bert
```
预处理的输出为dataset.pt，由--dataset_path指定 <br>
当使用多台机器进行预训练的时候，用一台机器执行preprocess.py，然后将得到的dataset.pt拷贝到其他机器之上<br>

我们需要在预处理阶段指定模型的目标（*--target*），因为不同的目标任务需要不同的数据格式。UER-py的目标任务由以下模块组成：
- lm：语言模型
- mlm：带有掩码的语言模型（完形填空）
- cls：分类
- bilm：双向语言模型
- bert：带有掩码的语言模型+下一个句子预测
- albert：带有掩码的语言模型+句子顺序预测

*--processes_num* 指定预处理进程的数量。BERT预处理非常慢，多进程能缓解预处理的速度问题。更多的进程数量会消耗更多的内存，可以根据内存选择合适的进程数量<br>
*--target* 指定预训练模型使用的目标任务，默认为bert。预处理和预训练阶段的target必须一致。Mlm目标任务在bert的基础上去掉了nsp任务；albert目标任务在bert的基础上将nsp任务替换为sop任务 <br>
*--dynamic_masking* 指定加入动态遮罩策略。动态遮罩在RoBERTa中被使用，指每个样本在训练时动态的进行遮罩 <br>
*--full_sentences* 表示目标任务为mlm时，允许样本包含跨文档的内容，直到最大长度。这一策略出自RoBERTa <br>
*--span_masking* 指定在mlm目标任务中加入区域遮罩策略。区域遮罩在SpanBERT中被使用，指每个样本在进行遮罩时，遮罩连续多个tokens。如果使用静态遮罩，则 --span_masking 在预处理阶段指定；如果使用动态遮罩，则 --span_masking 在预训练阶段指定 <br>
*--docs_buffer_size* 指定单进程缓存文档数量。更大的数量会消耗更多的内存，可以根据内存选择合适的缓存文档数量 <br>
*--seq_length* 指定预训练样本的最大长度。最大长度需要在预处理阶段指定，数值不超过512 <br>
在预处理阶段还指定了词汇和分词方式，更多内容将在*标记化和词汇*部分中讨论。
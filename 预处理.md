```
usage: preprocess.py [-h] --corpus_path CORPUS_PATH [--vocab_path VOCAB_PATH]
                     [--spm_model_path SPM_MODEL_PATH]
                     [--tgt_vocab_path TGT_VOCAB_PATH]
                     [--tgt_spm_model_path TGT_SPM_MODEL_PATH]
                     [--dataset_path DATASET_PATH]
                     [--tokenizer {bert,char,space}]
                     [--tgt_tokenizer {bert,char,space}]
                     [--processes_num PROCESSES_NUM]
                     [--target {bert,lm,mlm,bilm,albert,seq2seq,t5,cls,prefixlm}]
                     [--docs_buffer_size DOCS_BUFFER_SIZE]
                     [--seq_length SEQ_LENGTH]
                     [--tgt_seq_length TGT_SEQ_LENGTH]
                     [--dup_factor DUP_FACTOR]
                     [--short_seq_prob SHORT_SEQ_PROB] [--full_sentences]
                     [--seed SEED] [--dynamic_masking] [--whole_word_masking]
                     [--span_masking] [--span_geo_prob SPAN_GEO_PROB]
                     [--span_max_length SPAN_MAX_LENGTH]
```
用户必须在预训练之前对语料进行预处理。 <br>
在单台机器上进行预处理的示例：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                      --processes_num 8 --target bert
```
当使用多台机器进行预训练的时候，用一台机器执行*preprocess.py*，然后将得到的*dataset.pt*拷贝到其他机器之上。 <br>
预处理的输出为*dataset.pt*（*--dataset_path*），并作为*pretrain.py*的输入。 <br>


我们需要在预处理阶段指定模型的预训练目标（*--target*），因为不同的目标任务在预训练阶段需要不同的数据格式。UER-py包括如下的预训练目标：
- lm：语言模型
- mlm：遮罩语言模型（完形填空形式）
- cls：分类
- bilm：双向语言模型
- bert：遮罩语言模型+下一个句子预测
- albert：遮罩语言模型+句子顺序预测
- t5：遮罩语言模型（序列到序列形式）
- prefixlm：前缀语言模型
- seq2seq：序列到序列

注意到我们指定的语料（*--corpus_path*）的格式应该和指定的预训练目标匹配。更多的例子可以在[更多预训练模型](https://github.com/dbiir/UER-py/wiki/更多预训练模型)中找到。

*--processes_num* 指定预处理进程的数量。多进程能缓解预处理的速度问题，但是更多的进程会消耗更多的内存。可以根据内存和CPU选择合适的进程数量。 <br>
*--dup_factor* 指定样本复制的次数（当使用静态遮罩策略）。静态遮罩在BERT中被使用。哪些单词被遮罩在预处理阶段就被指定好。 <br>
*--dynamic_masking* 指定加入动态遮罩策略。动态遮罩在RoBERTa中被使用。这种遮罩方式一般效果更好，并且预处理生成的文件（*--dataset_path*）会更小，因为无需进行样本的复制。 <br>
*--full_sentences* 表示目标任务为mlm时，允许样本包含跨文档的内容，直到最大长度。这一策略出自RoBERTa。 <br>
*--span_masking* 指定加入区域遮罩策略。区域遮罩在SpanBERT中被使用，指每个样本在进行遮罩时，遮罩连续多个tokens。如果使用静态遮罩，则 *--span_masking* 在预处理阶段指定；如果使用动态遮罩，则 *--span_masking* 在预训练阶段指定。 <br>
*--docs_buffer_size* 指定单进程缓存文档数量。更大的数量会消耗更多的内存，可以根据内存选择合适的缓存文档数量。 <br>
*--seq_length* 指定预训练样本的最大长度。最大长度需要在预处理阶段指定。如果是加载已有的预训练模型权重进行增量预训练，*--seq_length* 不能超过模型支持的最大长度（*--max_seq_length*）。

在预处理阶段还需要指定词典和分词方式，更多内容在[分词和词典](https://github.com/dbiir/UER-py/wiki/分词和词典)中讨论。

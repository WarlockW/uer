```
usage: preprocess.py [-h] --corpus_path CORPUS_PATH [--vocab_path VOCAB_PATH]
                     [--spm_model_path SPM_MODEL_PATH]
                     [--dataset_path DATASET_PATH]
                     [--tokenizer {bert,char,space}]
                     [--processes_num PROCESSES_NUM]
                     [--target {bert,lm,cls,mlm,bilm,albert}]
                     [--docs_buffer_size DOCS_BUFFER_SIZE]
                     [--seq_length SEQ_LENGTH] [--dup_factor DUP_FACTOR]
                     [--short_seq_prob SHORT_SEQ_PROB] [--full_sentences]
                     [--seed SEED] [--dynamic_masking] [--span_masking]
                     [--span_geo_prob SPAN_GEO_PROB]
                     [--span_max_length SPAN_MAX_LENGTH]
```
用户必须在预训练之前对语料进行预处理。 <br>
在单台机器上进行预处理的示例：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt \
                      --processes_num 8 --target bert
```
当使用多台机器进行预训练的时候，用一台机器执行*preprocess.py*，然后将得到的*dataset.pt*拷贝到其他机器之上。 <br>
预处理的输出为*dataset.pt*（*--dataset_path*），并作为*pretrain.py*的输入。 <br>


我们需要在预处理阶段指定模型的预训练目标（*--target*），因为不同的目标任务需要不同的数据格式。UER-py包括如下的预训练目标：
- lm：语言模型
- mlm：遮罩语言模型（完形填空）
- cls：分类
- bilm：双向语言模型
- bert：遮罩语言模型+下一个句子预测
- albert：遮罩语言模型+句子顺序预测

注意到我们指定的语料（*--corpus_path*）的格式应该和指定的预训练目标匹配。更多的例子可以在[更多预训练模型](https://github.com/dbiir/UER-py/wiki/更多预训练模型)中找到。

*--processes_num* 指定预处理进程的数量。BERT预处理非常慢，多进程能缓解预处理的速度问题。更多的进程数量会消耗更多的内存，可以根据内存和CPU选择合适的进程数量。 <br>
*--dynamic_masking* 指定加入动态遮罩策略。动态遮罩在RoBERTa中被使用。 <br>
*--full_sentences* 表示目标任务为mlm时，允许样本包含跨文档的内容，直到最大长度。这一策略出自RoBERTa。 <br>
*--span_masking* 指定加入区域遮罩策略。区域遮罩在SpanBERT中被使用，指每个样本在进行遮罩时，遮罩连续多个tokens。如果使用静态遮罩，则 *--span_masking* 在预处理阶段指定；如果使用动态遮罩，则 *--span_masking* 在预训练阶段指定。 <br>
*--docs_buffer_size* 指定单进程缓存文档数量。更大的数量会消耗更多的内存，可以根据内存选择合适的缓存文档数量。 <br>
*--seq_length* 指定预训练样本的最大长度。最大长度需要在预处理阶段指定，数值不超过512。 <br>

在预处理阶段还需要指定词典和分词方式，更多内容在[分词和词典](https://github.com/dbiir/UER-py/wiki/分词和词典)中讨论。

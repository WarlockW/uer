
借助UER，我们使用不同的语料、编码器和目标任务进行了预训练。以下所有的预训练模型都是UER格式的，可以由UER直接加载。未来会发布更多的预训练模型。除非另有说明，否则中文预训练模型使用 *models/google_zh_vocab.txt* 作为词典（原始Google BERT项目中的中文词典）以及BERT tokenizer作为分词器。*models/bert/base_config.json* 为默认的配置文件；常用的词典和配置文件包含在 *models* 文件夹中，用户无需下载。此外，我们通过 *scripts/convert_xxx_from_uer_to_huggingface.py* 将UER预训练的模型转换为Huggingface Transformers支持的格式，并上传到了[Huggingface模型仓库（uer用户）](https://huggingface.co/uer)。下面介绍这些预训练模型权重，给出它们的下载链接，以及说明它们的使用方式。注意到，受限于篇幅，我们将预训练权重的细节描述放到了相应的Huggingface模型仓库中。在介绍具体预训练模型权重的时候，我们会给出其对应的Huggingface模型仓库链接。

## 中文RoBERTa预训练模型
24个不同尺寸的中文RoBERTa预训练模型。语料为CLUECorpusSmall。配置文件在 *models/bert/* 路径下。我们只为Tiny，Mini，Small，Medium，Base，Large模型提供了配置文件。为了加载下面的其他模型，我们需要修改配置文件中的 *emb_size*，*feedforward_size*，*hidden_size*，*heads_num*，*layers_num*。注意到*emb_size*等于*hidden_size*，*feedforward_size*是*hidden_size*的4倍，*heads_num*等于*hidden_size*除以64。更多的细节请参考[这里](https://huggingface.co/uer/chinese_roberta_L-2_H-128)。

下面列出不同层数 L（*layers_num*），不同隐层维度 H（*hidden_size*）的中文RoBERTa预训练权重链接：

|层数/隐层维度|           H=128           |           H=256           |            H=512            |            H=768            |
| -------- | :-----------------------: | :-----------------------: | :-------------------------: | :-------------------------: |
| **L=2**  | [**2/128 (Tiny)**][2_128] |      [2/256][2_256]       |       [2/512][2_512]        |       [2/768][2_768]        |
| **L=4**  |      [4/128][4_128]       | [**4/256 (Mini)**][4_256] | [**4/512 (Small)**][4_512]  |       [4/768][4_768]        |
| **L=6**  |      [6/128][6_128]       |      [6/256][6_256]       |       [6/512][6_512]        |       [6/768][6_768]        |
| **L=8**  |      [8/128][8_128]       |      [8/256][8_256]       | [**8/512 (Medium)**][8_512] |       [8/768][8_768]        |
| **L=10** |     [10/128][10_128]      |     [10/256][10_256]      |      [10/512][10_512]       |      [10/768][10_768]       |
| **L=12** |     [12/128][12_128]      |     [12/256][12_256]      |      [12/512][12_512]       | [**12/768 (Base)**][12_768] |

这里以Tiny预训练模型权重为例说明以上权重的使用方法。我们通过上面的链接下载Tiny预训练模型权重，放到 *models/* 文件夹下。我们可以在其基础上增量的预训练：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target mlm

python3 pretrain.py --dataset_path dataset.pt --pretrained_model_path models/cluecorpussmall_roberta_tiny_seq512_model.bin \
                    --vocab_path models/google_zh_vocab.txt --config_path models/bert/tiny_config.json \
                    --output_model_path models/output_model.bin --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 5000 --save_checkpoint_steps 2500 --batch_size 64 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```

或者用其进行分类：
```
python3 finetune/run_classifier.py --pretrained_model_path models/cluecorpussmall_roberta_tiny_seq512_model.bin \
                                   --vocab_path models/google_zh_vocab.txt --config_path models/bert/tiny_config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --learning_rate 3e-4 --batch_size 64 --epochs_num 8 \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```

在微调阶段，不同尺寸的预训练模型，通常需要不同的超参数。使用网格搜索寻找分类模型的最佳超参数示例：
```
python3 finetune/run_classifier_grid.py --pretrained_model_path models/cluecorpussmall_roberta_tiny_seq512_model.bin \
                                        --vocab_path models/google_zh_vocab.txt --config_path models/bert/tiny_config.json \
                                        --train_path datasets/douban_book_review/train.tsv \
                                        --dev_path datasets/douban_book_review/dev.tsv \
                                        --learning_rate_list 3e-5 1e-4 3e-4 --batch_size_list 32 64 --epochs_num_list 3 5 8 \
                                        --embedding word_pos_seg --encoder transformer --mask fully_visible
```
通过上面的网格搜索脚本，可以复现[这里](https://huggingface.co/uer/chinese_roberta_L-2_H-128)列出的实验结果。

## 基于词的中文RoBERTa预训练模型
5个不同尺寸的基于词的中文RoBERTa预训练模型。语料为CLUECorpusSmall。配置文件在 *models/bert/* 路径下。分词工具为Google sentencepiece，使用的sentencepiece模型为 *models/cluecorpussmall_spm.model* 。目前主流的中文预训练模型是基于字的。我们发现基于词的预训练模型在下游任务上往往有更好的效果，并且在推理速度上更有优势（由于更短的序列长度）。更多的细节请参考[这里](https://huggingface.co/uer/roberta-tiny-word-chinese-cluecorpussmall)。

下面列出不同尺寸的基于词的中文RoBERTa预训练权重链接：

|           模型链接           |
| :-----------------------:|
| [**L=2/H=128 (Tiny)**][word_tiny] |
| [**L=4/H=256 (Mini)**][word_mini] |
| [**L=4/H=512 (Small)**][word_small] |
| [**L=8/H=512 (Medium)**][word_medium] |
| [**L=12/H=768 (Base)**][word_base] |

这里以基于词的Tiny预训练模型权重为例说明以上权重的使用方法。我们通过上面的链接下载基于词的Tiny预训练模型权重，放到 *models/* 文件夹下。我们可以在其基础上增量的预训练：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --spm_model_path models/cluecorpussmall_spm.model \
                      --dataset_path dataset.pt --processes_num 8 --target mlm

python3 pretrain.py --dataset_path dataset.pt --pretrained_model_path models/cluecorpussmall_roberta_tiny_seq512_model.bin \
                    --spm_model_path models/cluecorpussmall_spm.model --config_path models/bert/tiny_config.json \
                    --output_model_path models/output_model.bin --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 5000 --save_checkpoint_steps 2500 --batch_size 64 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```

或者用其进行分类：
```
python3 finetune/run_classifier.py --pretrained_model_path models/cluecorpussmall_word_roberta_tiny_seq512_model.bin \
                                   --spm_model_path models/cluecorpussmall_spm.model \
                                   --config_path models/bert/tiny_config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --learning_rate 3e-4 --batch_size 64 --epochs_num 8 \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```

使用网格搜索寻找基于词的分类模型的最佳超参数示例：
```
python3 finetune/run_classifier_grid.py --pretrained_model_path models/cluecorpussmall_word_roberta_tiny_seq512_model.bin \
                                        --spm_model_path models/cluecorpussmall_spm.model --config_path models/bert/tiny_config.json \
                                        --train_path datasets/douban_book_review/train.tsv --dev_path datasets/douban_book_review/dev.tsv \
                                        --learning_rate_list 3e-5 1e-4 3e-4 --batch_size_list 32 64 --epochs_num_list 3 5 8 \
                                        --embedding word_pos_seg --encoder transformer --mask fully_visible
```
通过上面的网格搜索脚本，可以复现[这里](https://huggingface.co/uer/roberta-tiny-word-chinese-cluecorpussmall)列出的实验结果。

## 基于不同语料的中文GPT-2预训练模型
我们基于不同的语料，训练了一系列GPT-2语言模型。配置文件在 *models/gpt2/* 路径下。下面列出它们的权重链接和细节描述链接（Huggingface模型仓库）：

|           模型链接           |           细节描述链接           |
| :-----------------------:| :-----------------------:|
| [**通用中文GPT-2预训练模型**][gpt2_cluecorpussmall] | https://huggingface.co/uer/gpt2-chinese-cluecorpussmall |
| [**通用中文GPT-2预训练小模型**][gpt2_distil_cluecorpussmall] | https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall |
| [**古诗词GPT-2预训练模型**][gpt2_poem] | https://huggingface.co/uer/gpt2-chinese-poem |
| [**对联GPT-2预训练模型**][gpt2_couplet] | https://huggingface.co/uer/gpt2-chinese-couplet |
| [**中文歌词GPT-2预训练模型**][gpt2_lyric] | https://huggingface.co/uer/gpt2-chinese-lyric |
| [**文言文GPT-2预训练模型**][gpt2_ancient] | https://huggingface.co/uer/gpt2-chinese-ancient |

需要注意的是，古诗词和文言文模型使用了扩展的词典（分别为*models/google_zh_poem_vocab.txt*和*models/google_zh_ancient_vocab.txt*）。通用中文GPT-2预训练小模型使用的配置文件是 *models/gpt2/distil_config.json* ,其余的预训练权重使用的配置文件是 *models/gpt2/config.json* 。

这里以通用中文GPT-2预训练小模型权重为例说明以上权重的使用方法。我们通过上面的链接下载通用中文GPT-2预训练小模型权重，放到 *models/* 文件夹下。我们可以在其基础上增量的预训练：
```
python3 preprocess.py --corpus_path corpora/book_review.txt \
                      --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --seq_length 128 --target lm 

python3 pretrain.py --dataset_path dataset.pt \
                    --pretrained_model_path models/cluecorpussmall_gpt2_distil_seq1024_model.bin \
                    --vocab_path models/google_zh_vocab.txt \
                    --config_path models/gpt2/distil_config.json \
                    --output_model_path models/book_review_gpt2_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 10000 --save_checkpoint_steps 5000 --report_steps 1000 \
                    --learning_rate 5e-5 --batch_size 64 \
                    --embedding word_pos --remove_embedding_layernorm \
                    --encoder transformer --mask causal --layernorm_positioning pre \
                    --target lm --tie_weights
```
或者用其进行分类：
```
python3 finetune/run_classifier.py --pretrained_model_path models/cluecorpussmall_gpt2_distil_seq1024_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/gpt2/distil_config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --learning_rate 3e-5 --batch_size 64 --epochs_num 8 \
                                   --embedding word_pos_seg --remove_embedding_layernorm \
                                   --encoder transformer --mask causal --layernorm_positioning pre
```

我们可以通过GPT-2模型进行文本生成。首先创建 *story_beginning.txt* ，在里面输入文本的开头，然后利用 *scripts/* 文件夹下的 *generate_lm.py* 脚本进行文本生成：
```
python3 scripts/generate_lm.py --load_model_path models/cluecorpussmall_gpt2_distil_seq1024_model.bin \
                               --vocab_path models/google_zh_vocab.txt \
                               --config_path models/gpt2/distil_config.json --seq_length 128 \
                               --test_path story_beginning.txt --prediction_path story_full.txt \
                               --embedding word_pos --remove_embedding_layernorm \
                               --encoder transformer --mask causal --layernorm_positioning pre \
                               --target lm --tie_weights
```

## 中文T5预训练模型
我们基于CLUECorpusSmall语料，训练了一系列T5预训练模型。配置文件在 *models/t5/* 路径下。下面列出它们的权重链接和细节描述链接（Huggingface模型仓库）：

|           模型链接           |           细节描述链接           |
| :-----------------------:| :-----------------------:|
| [**通用中文T5-small预训练模型**][t5_small] | https://huggingface.co/uer/t5-small-chinese-cluecorpussmall |
| [**通用中文T5-base预训练模型**][t5_base] | https://huggingface.co/uer/t5-base-chinese-cluecorpussmall |

这里以通用中文T5-small预训练模型权重为例说明以上权重的使用方法。我们通过上面的链接下载通用中文T5-small预训练模型权重，放到 *models/* 文件夹下。我们可以在其基础上增量的预训练：
```
python3 preprocess.py --corpus_path corpora/book_review.txt \
                      --vocab_path models/google_zh_with_sentinel_vocab.txt \
                      --dataset_path dataset.pt \
                      --processes_num 8 --seq_length 128 \
                      --dynamic_masking --target t5

python3 pretrain.py --dataset_path dataset.pt \
                    --vocab_path models/google_zh_with_sentinel_vocab.txt \
                    --config_path models/t5/small_config.json \
                    --output_model_path models/book_review_t5_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 10000 --save_checkpoint_steps 5000 --report_steps 1000 \
                    --learning_rate 5e-4 --batch_size 64 \
                    --span_masking --span_geo_prob 0.3 --span_max_length 5 \
                    --embedding word --relative_position_embedding --remove_embedding_layernorm --tgt_embedding word \
                    --encoder transformer --mask fully_visible --layernorm_positioning pre --decoder transformer \
                    --target t5 --tie_weights
```

## 中文RoBERTa下游任务微调模型
我们基于RoBERTa预训练模型，在一系列下游任务上进行了微调。这些模型的配置文件均为 *models/bert/base_config.json* 。下面列出它们的权重链接和细节描述链接（Huggingface模型仓库）：

|           模型链接           |           细节描述链接           |
| :-----------------------:| :-----------------------:|
| [**JD full 情感分类**][roberta_jd_full_classification] | https://huggingface.co/uer/roberta-base-finetuned-jd-full-chinese |
| [**JD binary 情感分类**][roberta_jd_binary_classification] | https://huggingface.co/uer/roberta-base-finetuned-jd-binary-chinese |
| [**Dianping 情感分类**][roberta_dianping_classification] | https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese |
| [**Ifeng 新闻主题分类**][roberta_ifeng_classification] | https://huggingface.co/uer/roberta-base-finetuned-ifeng-chinese |
| [**Chinanews 新闻主题分类**][roberta_chinanews_classification] | https://huggingface.co/uer/roberta-base-finetuned-chinanews-chinese |
| [**CLUENER2020 命名实体识别**][roberta_cluener2020_token_classification] | https://huggingface.co/uer/roberta-base-finetuned-cluener2020-chinese |
| [**抽取式问答**][roberta_extractive_qa] | https://huggingface.co/uer/roberta-base-chinese-extractive-qa |

可以加载上面的模型进行增量预训练、微调、推理。

## 中文Transformer之外的预训练模型
我们基于CLUECorpusSmall语料，训练了一系列Transformer之外的预训练模型。下面列出它们的权重链接和细节描述：

|           模型链接           |           配置文件           |           模型结构细节           |           训练细节           |
| :-----------------------:| :-----------------------:| :-----------------------:| :-----------------------:|
| [**通用中文LSTM语言模型**][lstm_lm] | models/rnn_config.json | --embedding word --remove_embedding_layernorm --encoder lstm --target lm | 步数：500000 学习率：1e-3 batch size：64*8（GPU数量） 长度：256 |
| [**通用中文GRU语言模型**][gru_lm] | models/rnn_config.json | --embedding word --remove_embedding_layernorm --encoder gru --target lm | 步数：500000 学习率：1e-3 batch size：64*8（GPU数量） 长度：256 |
| [**通用中文GatedCNN语言模型**][gatedcnn_lm] | models/gatedcnn_9_config.json | --embedding word --remove_embedding_layernorm --encoder gatedcnn --target lm | 步数：500000 学习率：1e-4 batch size：64*8（GPU数量） 长度：256 |
| [**通用中文ELMo预训练模型**][elmo] | models/birnn_config.json | --embedding word --remove_embedding_layernorm --encoder bilstm --target bilm | 步数：500000 学习率：5e-4 batch size：64*8（GPU数量） 长度：256 |

## 其他机构中文预训练模型

|           模型链接           |           描述           |           细节描述链接           |
| :-----------------------:| :-----------------------:| :-----------------------:|
| [**Google Chinese BERT-Base**][google_zh_bert_base] | 配置文件：*models/bert/base_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/google-research/bert |
| [**Google Chinese ALBERT-Base**][google_zh_albert_base] | 配置文件：*models/albert/base_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/google-research/albert |
| [**Google Chinese ALBERT-Large**][google_zh_albert_large] | 配置文件：*models/albert/large_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/google-research/albert |
| [**Google Chinese ALBERT-Xlarge**][google_zh_albert_xlarge] | 配置文件：*models/albert/xlarge_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/google-research/albert |
| [**Google Chinese ALBERT-Xxlarge**][google_zh_albert_xxlarge] | 配置文件：*models/albert/xxlarge_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/google-research/albert |
| [**HFL Chinese BERT-wwm**][hfl_zh_bert_wwm] | 配置文件：*models/bert/base_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/ymcui/Chinese-BERT-wwm |
| [**HFL Chinese BERT-wwm-ext**][hfl_zh_bert_wwm_ext] | 配置文件：*models/bert/base_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/ymcui/Chinese-BERT-wwm |
| [**HFL Chinese RoBERTa-wwm-ext**][hfl_zh_roberta_wwm_ext] | 配置文件：*models/bert/base_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/ymcui/Chinese-BERT-wwm |
| [**HFL Chinese RoBERTa-wwm-large-ext**][hfl_zh_roberta_wwm_large_ext] | 配置文件：*models/bert/large_config.json* <br> 词典：*models/google_zh_vocab.txt* <br> 分词器：BertTokenizer | https://github.com/ymcui/Chinese-BERT-wwm |

## 其他预训练模型

UER预训练模型
<table>
<tr align="center"><th> 预训练模型 <th> Link <th> 描述 
<tr align="center"><td> Wikizh(word-based)+BertEncoder+BertTarget <td> 模型：https://share.weiyun.com/5s4HVMi 词典：https://share.weiyun.com/5NWYbYn <td> 在中文维基百科语料上训练的基于词的BERT模型，训练步数：50万步。
<tr align="center"><td> RenMinRiBao+BertEncoder+BertTarget <td> https://share.weiyun.com/5JWVjSE <td> 训练语料为人民日报，适合用于新闻相关的数据集。
<tr align="center"><td> Webqa2019+BertEncoder+BertTarget <td> https://share.weiyun.com/5HYbmBh <td> 训练语料为社区问答，包含问题，问题描述，答案，适合社交场景相关的数据集，训练步数：50万步。
<tr align="center"><td> Weibo+BertEncoder+BertTarget <td> https://share.weiyun.com/5ZDZi4A <td> 训练语料为微博。
<tr align="center"><td> Weibo+BertEncoder(large)+MlmTarget <td> https://share.weiyun.com/CFKyMkp3 <td> 训练语料为微博。配置文件为bert_large_config.json
<tr align="center"><td> Reviews+BertEncoder+MlmTarget <td> https://share.weiyun.com/tBgaSx77 <td> 训练语料为评论。
<tr align="center"><td> Reviews+BertEncoder(large)+MlmTarget <td> https://share.weiyun.com/hn7kp9bs <td> 训练语料为评论，配置文件为 bert_large_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(xlarge)+MlmTarget <td> https://share.weiyun.com/J9rj9WRB <td> 训练语料为大规模混合语料，配置文件是bert_xlarge_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(xlarge)+BertTarget(WWM) <td> https://share.weiyun.com/UsI0OSeR <td> 训练语料为大规模混合语料，配置文件是bert_xlarge_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(large)+BertTarget <td> https://share.weiyun.com/5G90sMJ <td> 训练语料为大规模混合语料，配置文件是bert_large_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(base)+BertTarget <td> https://share.weiyun.com/5QOzPqq <td> 训练语料为大规模混合语料，配置文件是bert_base_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(small)+BertTarget <td> https://share.weiyun.com/fhcUanfy <td> 训练语料为大规模混合语料，配置文件是bert_small_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(tiny)+BertTarget <td> https://share.weiyun.com/yXx0lfUg <td> 训练语料为大规模混合语料，配置文件是bert_tiny_config.json
<tr align="center"><td> MixedCorpus+GptEncoder+LmTarget <td> https://share.weiyun.com/51nTP8V <td> 训练语料为大规模混合语料，训练步数为50万 (句长128) + 10万 (句长512)
<tr align="center"><td> Reviews+LstmEncoder+LmTarget <td> https://share.weiyun.com/57dZhqo  <td> 训练语料是亚马逊网站评论 + 京东评论 + 点评 (共11.4M 评论)，使用语言模型作为目标任务。适用于与评论相关的数据集。与随机初始化相比，它在某些评论数据集上实现了5％以上的改进。在使用它之前，将models/rnn_config.json中的hidden_size设置为512。训练步数：200,000；句长：128。
<tr align="center"><td> (MixedCorpus & Amazon reviews)+LstmEncoder+(LmTarget & ClsTarget) <td> https://share.weiyun.com/5B671Ik  <td> 首先以LM作为目标任务对混合中文大语料库进行预训练，然后使用lm目标任务和cls目标任务对亚马逊评论进行预训练。它适用于与评论相关的数据集。在某些评论数据集上，它可以与BERT取得可比的结果。 训练步数：500,000 + 100,000； 句长：128
<tr align="center"><td> IfengNews+BertEncoder+BertTarget <td> https://share.weiyun.com/5HVcUWO <td> 训练语料是来自Ifeng网站的新闻数据，使用新闻标题来预测新闻摘要。训练步数：100,000； 句长：128
<tr align="center"><td> jdbinary+BertEncoder+ClsTarget <td> https://share.weiyun.com/596k2bu <td> 训练语料是京东的审查数据，以cls作为目标任务进行预训练。它适用于与购物评论相关的数据集。训练步数：50,000；句长：128
<tr align="center"><td> jdfull+BertEncoder+MlmTarget <td> https://share.weiyun.com/5L6EkUF <td> 训练语料是京东的审查数据，以mlm作为目标任务进行预训练。训练步数：50,000；句长：128
<tr align="center"><td> Amazonreview+BertEncoder+ClsTarget <td> https://share.weiyun.com/5XuxtFA <td> 训练语料是来自亚马逊网站的评论数据（包括书评，电影评论等），以cls作为目标任务进行预训练。它适用于与评论相关的数据集，例如与Google BERT相比，该模型将豆瓣图书评论数据集的准确性从87.6提高到88.5。 训练步数：20,000； 句长：128
<tr align="center"><td> XNLI+BertEncoder+ClsTarget <td> https://share.weiyun.com/5oXPugA <td> 用BertEncoder进行推理
</table>
MixedCorpus包含百度百科，中文维基百科，网站问答，人民日报，文献和评论。
<br/>

[2_128]:https://share.weiyun.com/5mXkEvxN
[2_256]:https://share.weiyun.com/TLn5VpSW
[2_512]:https://share.weiyun.com/nYtFjDBL
[2_768]:https://share.weiyun.com/sHMGMv4c
[4_128]:https://share.weiyun.com/0SlDbnSM
[4_256]:https://share.weiyun.com/0eNSFKw9
[4_512]:https://share.weiyun.com/SEJ0MStj
[4_768]:https://share.weiyun.com/jk42uruV
[6_128]:https://share.weiyun.com/tWeXueMJ
[6_256]:https://share.weiyun.com/1Ku03xFM
[6_512]:https://share.weiyun.com/WEvmJUOD
[6_768]:https://share.weiyun.com/m5AMthBU
[8_128]:https://share.weiyun.com/r0M1pJsV
[8_256]:https://share.weiyun.com/xk1SN8V9
[8_512]:https://share.weiyun.com/vpnaZ5jp
[8_768]:https://share.weiyun.com/NT67VIap
[10_128]:https://share.weiyun.com/dPKrelIL
[10_256]:https://share.weiyun.com/dbn2bG0t
[10_512]:https://share.weiyun.com/q8yIZWje
[10_768]:https://share.weiyun.com/dz4sgiCx
[12_128]:https://share.weiyun.com/93UYnnSC
[12_256]:https://share.weiyun.com/czAR5KNu
[12_512]:https://share.weiyun.com/gv4zARxk
[12_768]:https://share.weiyun.com/2rEWrSQz

[word_tiny]:https://share.weiyun.com/6mUaN18A
[word_mini]:https://share.weiyun.com/og1Km7qM
[word_small]:https://share.weiyun.com/SqbCfIgp
[word_medium]:https://share.weiyun.com/yDz44dlS
[word_base]:https://share.weiyun.com/5OXC8Rzt

[gpt2_cluecorpussmall]:https://share.weiyun.com/0eAlQWRB
[gpt2_distil_cluecorpussmall]:https://share.weiyun.com/IAvDbjKR
[gpt2_poem]:https://share.weiyun.com/DKAmuOLU
[gpt2_couplet]:https://share.weiyun.com/LbMecOGj
[gpt2_lyric]:https://share.weiyun.com/jBv5weES
[gpt2_ancient]:https://share.weiyun.com/d3FHbVMx

[t5_small]:https://share.weiyun.com/uRNl4CXz
[t5_base]:https://share.weiyun.com/QvVu3w5a

[roberta_jd_full_classification]:https://share.weiyun.com/paIBCwA3
[roberta_jd_binary_classification]:https://share.weiyun.com/f9a3HUES
[roberta_dianping_classification]:https://share.weiyun.com/B9ceqrhh
[roberta_ifeng_classification]:https://share.weiyun.com/awR9YP7S
[roberta_chinanews_classification]:https://share.weiyun.com/imuT6FuN

[roberta_cluener2020_token_classification]:https://share.weiyun.com/FYyEsX8B
[roberta_extractive_qa]:https://share.weiyun.com/vNsCcyuo

[google_zh_bert_base]:https://share.weiyun.com/A1C49VPb
[google_zh_albert_base]:https://share.weiyun.com/UnKHNKRG
[google_zh_albert_large]:https://share.weiyun.com/9tTUwALd
[google_zh_albert_xlarge]:https://share.weiyun.com/mUamRQFR
[google_zh_albert_xxlarge]:https://share.weiyun.com/0i2lX62b

[hfl_zh_bert_wwm]:https://share.weiyun.com/rgOiCvGj
[hfl_zh_bert_wwm_ext]:https://share.weiyun.com/IxirLuvk
[hfl_zh_roberta_wwm_ext]:https://share.weiyun.com/b2Pp9jdR
[hfl_zh_roberta_wwm_large_ext]:https://share.weiyun.com/bE8SUvoM

[lstm_lm]:https://share.weiyun.com/XFc4hcn6
[gru_lm]:https://share.weiyun.com/2m6WRATo
[gatedcnn_lm]:https://share.weiyun.com/VLe8O6kM
[elmo]:https://share.weiyun.com/xezGTd86
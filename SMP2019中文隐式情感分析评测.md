这里说明如何在SMP2019-ECISA数据集上进行stacking集成，从而得到SOTA结果。更多的关于模型集成的说明请参考[这里](https://github.com/dbiir/UER-py/wiki/SMP2020微博情绪分类技术评测)。SMP2019-ECISA数据集可以在下游任务数据集章节中下载。首先使用K折交叉验证训练，得到分类器和训练集特征：
```
CUDA_VISIBLE_DEVICES=0,1 python3 run_classifier_cv.py --pretrained_model_path models/google_zh_model.bin \
                                                      --vocab_path models/google_zh_vocab.txt \
                                                      --output_model_path models/ecisa_classifier_model_0.bin \
                                                      --config_path models/bert/base_config.json \
                                                      --train_path datasets/smp2019-ecisa/train.tsv \
                                                      --train_features_path datasets/smp2019-ecisa/train_features_0.npy \
                                                      --folds_num 5 --epochs_num 3 --batch_size 32 \
                                                      --embedding word_pos_seg --encoder transformer --mask fully_visible

CUDA_VISIBLE_DEVICES=0,1 python3 run_classifier_cv.py --pretrained_model_path models/review_roberta_large_model.bin \
                                                      --vocab_path models/google_zh_vocab.txt \
                                                      --output_model_path models/ecisa_classifier_model_1.bin \
                                                      --config_path models/bert/large_config.json \
                                                      --train_path datasets/smp2019-ecisa/train.tsv \
                                                      --train_features_path datasets/smp2019-ecisa/train_features_1.npy \
                                                      --folds_num 5 --epochs_num 3 --batch_size 32 \                                                      
                                                      --embedding word_pos_seg --encoder transformer --mask fully_visible

CUDA_VISIBLE_DEVICES=7 python3 run_classifier_cv.py --pretrained_model_path models/cluecorpussmall_roberta_base_seq512_model.bin-250000 \
                                                    --vocab_path models/google_zh_vocab.txt \
                                                    --output_model_path models/ecisa_classifier_model_2.bin \
                                                    --config_path models/bert/base_config.json \
                                                    --train_path datasets/smp2019-ecisa/train.tsv \
                                                    --train_features_path datasets/smp2019-ecisa/train_features_2.npy \
                                                    --folds_num 5 --epochs_num 3 --batch_size 32 --seq_length 160 \
                                                    --embedding word_pos_seg --encoder transformer --mask fully_visible

CUDA_VISIBLE_DEVICES=0 python3 run_classifier_cv.py --pned_model_path models/cluecorpussmall_gpt2_seq1024_model.bin-250000 \
                                                    --vocab_path models/google_zh_vocab.txt \
                                                    --output_model_path models/ecisa_classifier_model_3.bin \
                                                    --config_path models/gpt2/config.json \
                                                    --train_path datasets/smp2019-ecisa/train.tsv \
                                                    --train_features_path datasets/smp2019-ecisa/train_features_3.npy \
                                                    --folds_num 5 --epochs_num 3 --batch_size 32 --seq_length 100 \
                                                    --embedding word_pos --remove_embedding_layernorm \
                                                    --encoder transformer --mask causal --layernorm_position pre --pooling mean

CUDA_VISIBLE_DEVICES=6,7 python3 run_classifier_cv.py --pretrained_model_path models/mixed_corpus_bert_large_model.bin \
                                                       --vocab_path models/google_zh_vocab.txt \
                                                       --output_model_path models/ecisa_classifier_model_4.bin \
                                                       --config_path models/bert/large_config.json \
                                                       --train_path datasets/smp2019-ecisa/train.tsv \
                                                       --train_features_path datasets/smp2019-ecisa/train_features_4.npy \
                                                       --folds_num 5 --epochs_num 3 --batch_size 32 \
                                                       --embedding word_pos_seg --encoder transformer --mask fully_visible
```

使用K折交叉验证推理，得到验证集特征：
```
CUDA_VISIBLE_DEVICES=0 python3 inference/run_classifier_infer_cv.py --load_model_path models/ecisa_classifier_model_0.bin \
                                                                    --vocab_path models/google_zh_vocab.txt \
                                                                    --config_path models/bert/base_config.json \
                                                                    --test_path datasets/smp2019-ecisa/dev.tsv \
                                                                    --test_features_path datasets/smp2019-ecisa/test_features_0.npy \
                                                                    --folds_num 5 --labels_num 3 \
                                                                    --embedding word_pos_seg --encoder transformer --mask fully_visible

CUDA_VISIBLE_DEVICES=0 python3 inference/run_classifier_infer_cv.py --load_model_path models/ecisa_classifier_model_1.bin \
                                                                    --vocab_path models/google_zh_vocab.txt \
                                                                    --config_path models/bert/large_config.json \
                                                                    --test_path datasets/smp2019-ecisa/dev.tsv \
                                                                    --test_features_path datasets/smp2019-ecisa/test_features_1.npy \
                                                                    --folds_num 5 --labels_num 3 \
                                                                    --embedding word_pos_seg --encoder transformer --mask fully_visible

CUDA_VISIBLE_DEVICES=0 python3 inference/run_classifier_infer_cv.py --load_model_path models/ecisa_classifier_model_2.bin \
                                                                    --vocab_path models/google_zh_vocab.txt \
                                                                    --config_path models/bert/base_config.json \
                                                                    --test_path datasets/smp2019-ecisa/dev.tsv \
                                                                    --test_features_path datasets/smp2019-ecisa/test_features_2.npy \
                                                                    --folds_num 5 --labels_num 3 \
                                                                    --embedding word_pos_seg --encoder transformer --mask fully_visible

CUDA_VISIBLE_DEVICES=0 python3 inference/run_classifier_infer_cv.py --load_model_path models/ecisa_classifier_model_3.bin \
                                                                    --vocab_path models/google_zh_vocab.txt \
                                                                    --config_path models/gpt2/config.json \
                                                                    --test_path datasets/smp2019-ecisa/dev.tsv \
                                                                    --test_features_path datasets/smp2019-ecisa/test_features_3.npy \
                                                                    --folds_num 5 --labels_num 3 --seq_length 100 \
                                                                    --embedding word_pos --remove_embedding_layernorm \
                                                                    --encoder transformer --mask causal --layernorm_position pre --pooling mean

CUDA_VISIBLE_DEVICES=0 python3 inference/run_classifier_infer_cv.py --load_model_path models/ecisa_classifier_model_4.bin \
                                                                    --vocab_path models/google_zh_vocab.txt \
                                                                    --config_path models/bert/large_config.json \
                                                                    --test_path datasets/smp2019-ecisa/dev.tsv \
                                                                    --test_features_path datasets/smp2019-ecisa/test_features_4.npy \
                                                                    --folds_num 5 --labels_num 3 \
                                                                    --embedding word_pos_seg --encoder transformer --mask fully_visible
```

LightGBM超参数搜索：
```
python3 scripts/run_lgb_cv_bayesopt.py --train_path datasets/smp2019-ecisa/train.tsv \
                                       --train_features_path datasets/smp2019-ecisa/ \
                                       --models_num 5 --folds_num 5 --labels_num 3 --epochs_num 100
```

使用LightGBM进行训练和验证：
```
python3 scripts/run_lgb.py --train_path datasets/smp2019-ecisa/train.tsv --test_path datasets/smp2019-ecisa/dev.tsv \
                           --train_features_path datasets/smp2019-ecisa/ --test_features_path datasets/smp2019-ecisa/ \
                           --models_num 5 --labels_num 3
```



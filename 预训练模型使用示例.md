在预处理和预训练过程中，可以通过选择不同的模块进行组合（比如词向量模块、编码器模块、预训练目标模块）得到不同的预训练模型。下面给出了实现常用预训练模型的相关示例。

### RoBERTa
RoBERTa预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --dynamic_masking --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
RoBERTa使用动态遮罩策略，mlm目标任务，并且允许样本包括跨文档的内容。 <br>
如果预训练的文档比较短，则推荐不使用 *--full_sentences* 选项。 <br>
注意到RoBERTa去掉了句子预测任务，因此输入的语料格式是一行一个文档，与BERT要求的语料格式不同。在上面的示例中，预处理加载的语料是*corpora/book_review.txt*，而不是*corpora/book_review_bert.txt*。 <br>
RoBERTa可以加载已有的BERT模型进行增量预训练（反之亦然）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --dynamic_masking --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --pretrained_model_path models/google_zh_model.bin \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 2e-5 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```

### ALBERT
ALBERT预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target albert

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/albert/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --factorized_embedding_parameterization --parameter_sharing \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target albert
```
ALBERT使用的语料的格式和BERT一样。 <br>
*--target albert* 表示使用ALBERT的目标任务，包含了遮罩语言模型和句子顺序预测任务。<br>
*--factorized_embedding_parameterization* 表示词向量层分解。<br>
*--parameter_sharing* 表示编码层参数共享。<br>
我们提供了4种ALBERT模型的配置文件，并放在 *models/albert/* 文件夹下，分别是 base_config.json , large_config.json , xlarge_config.json , xxlarge_config.json 。<br>
加载Google中文ALBERT预训练模型进行增量预训练示例（从预训练模型仓库章节可以下载转换好的权重）：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target albert

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --pretrained_model_path models/google_zh_albert_base_model.bin \
                    --output_model_path models/output_model.bin \
                    --config_path models/albert/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 2e-5 \
                    --factorized_embedding_parameterization --parameter_sharing \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target albert

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --pretrained_model_path models/google_zh_albert_xlarge_model.bin \
                    --output_model_path models/output_model.bin \
                    --config_path models/albert/xlarge_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 2e-5 \
                    --factorized_embedding_parameterization --parameter_sharing \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target albert
```

### SpanBERT
SpanBERT除了引入区域遮罩，还引入了span boundary目标任务。这里我们只考虑区域遮罩。<br>
SpanBERT预处理和预训练示例（静态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --dup_factor 20 \
                      --span_masking --span_geo_prob 0.3 --span_max_length 5 --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
*--dup_factor* 指定语料复制的次数（每次使用不同的遮罩），默认复制5次。<br>
SpanBERT预处理和预训练示例（动态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --dynamic_masking --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --span_masking --span_geo_prob 0.3 --span_max_length 5 \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```

### BERT-WWM
BERT-WWM对整词进行遮罩。这里我们使用MLM预训练目标任务。
BERT-WWM预处理和预训练示例（静态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt \
                      --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt \
                      --processes_num 8 --dup_factor 20 \
                      --whole_word_masking \
                      --target mlm

python3 pretrain.py --dataset_path dataset.pt \
                    --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
*--whole_word_masking* 指定使用整词遮罩。
BERT-WWM预处理和预训练示例（动态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt \
                      --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt \
                      --processes_num 8 --dynamic_masking \
                      --target mlm

python3 pretrain.py --dataset_path dataset.pt \
                    --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --whole_word_masking \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
UER中实现的BERT-WMM是针对中文的。[jieba](https://github.com/fxsjy/jieba)被用作分词工具（具体参见*uer/utils/data.py*）:
```
import jieba
wordlist = jieba.cut(sentence)
```
可以通过修改 *uer/utils/data.py* 中的代码将分词工具由jieba替换为其他分词工具。

### GPT
GPT预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/gpt2/config.json --learning_rate 1e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --embedding word_pos --encoder transformer --mask causal --target lm
```
GPT使用的语料的格式和RoBERTa一样。我们可以通过指定 *--embedding word_pos --encoder transformer --mask causal --target lm* 复现预训练GPT模型。

### GPT-2
GPT-2预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/gpt2/config.json --learning_rate 1e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --embedding word_pos --remove_embedding_layernorm \
                    --encoder transformer --mask causal --layernorm_positioning pre \
                    --target lm --tie_weights
```
GPT-2使用的语料的格式和GPT、RoBERTa一样。GPT-2的编码器不同于GPT的编码器，使用了前置layernorm（*--layernorm_positioning pre*），并在编码层的最后加入了一个额外的layernorm。此外，我们需要去掉接在embedding层后面的layernorm（*--remove_embedding_layernorm*）。

### ELMo
ELMo预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target bilm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt  \
                    --output_model_path models/output_model.bin \
                    --config_path models/birnn_config.json --learning_rate 5e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --embedding word --remove_embedding_layernorm --encoder bilstm --target bilm
```
ELMo使用的语料的格式和GPT-2一样。我们可以通过*--embedding word*，*--encoder bilstm* 和 *--target bilm*的组合来预训练ELMo。<br>
*--embedding word* 表示使用传统的词向量层，LSTM不需要位置向量。此外，我们加上 *--remove_embedding_layernorm* ，去掉了embedding层后面的layernorm。

### T5
T5提出使用序列到序列的方式统一处理自然语言理解和自然语言生成任务。T5充分探索了预训练相关的技术并进行了系统的对比，推荐使用编码器-解码器结构以及BERT-style预训练目标（模型对被遮罩的单词进行预测）。T5预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --seq_length 128 \
                      --dynamic_masking --target t5

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --config_path models/t5/small_config.json \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --learning_rate 1e-3 --batch_size 64 \
                    --span_masking --span_geo_prob 0.3 --span_max_length 5 \
                    --embedding word --relative_position_embedding --remove_embedding_layernorm --tgt_embedding word \
                    --encoder transformer --mask fully_visible --layernorm_positioning pre --decoder transformer \
                    --target t5 --tie_weights
```
T5使用的语料的格式和GPT-2一样。我们通过 *--relative_position_embedding* 指定使用相对位置编码；通过 *--remove_embedding_layernorm* 和 *--layernorm_positioning pre* 指定使用前置layernorm（和GPT-2一样）。由于T5使用编码器-解码器结构，因此我们需要同时指定 *--encoder* 和 *--decoder* 。

### T5-v1_1
T5-v1_1在T5的基础上进行了多项的改进。T5-v1_1预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --seq_length 128 \
                      --dynamic_masking --target t5

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --config_path models/t5-v1_1/small_config.json \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --learning_rate 1e-3 --batch_size 64 \
                    --span_masking --span_geo_prob 0.3 --span_max_length 5 \
                    --embedding word --relative_position_embedding --remove_embedding_layernorm --tgt_embedding word \
                    --encoder transformer --mask fully_visible --layernorm_positioning pre --feed_forward gated --decoder transformer \
                    --target t5
```
T5-v1_1使用的语料的格式和T5一样。通过 *--feed_forward* 指定feed-forward层的类型。*--tie_weights* 被去掉，这样embedding层和softmax前一层不共享参数。此外，T5-v1_1和T5有着不同的配置文件。

### PEGASUS
PEGASUS提出使用GSG（gap sentence generation）目标任务预训练。GSG训练目标把文档中一部分的句子抽取出来进行预测，对文本摘要下游任务效果提升有一定的帮助。PEGASUS预处理和预训练示例：
```
python3  preprocess.py --corpus_path corpora/CLUECorpusSmall_5000_lines_bert.txt --vocab_path models/google_zh_vocab.txt \
                       --dataset_path dataset.pt --processes_num 8 --seq_length 512 --tgt_seq_length 256 \
                       --dup_factor 1 --target gsg --sentence_selection_stratege lead

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --config_path models/pegasus/base_config.json \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --learning_rate 1e-4 --batch_size 8 \
                    --embedding word_sinusoidalpos --tgt_embedding word_sinusoidalpos --remove_embedding_layernorm \
                    --encoder transformer --mask fully_visible --layernorm_positioning pre --decoder transformer \
                    --target gsg --has_lmtarget_bias --tie_weights
```
PEGASUS使用的语料的格式和BERT一样。在预处理阶段可以通过 *--sentence_selection_strategy* 指定句子选择的策略。如果使用随机句子选择策略（*--sentence_selection_strategy random*），可以使用 *--dup_factor* 指定语料复制的次数（每次选择不同的句子）。如果使用 *--sentence_selection_strategy lead* 句子选择策略，则 *--dup_factor* 设置为1。

### BART
BART提出使用seq2seq模型对任意噪声函数破坏的文本进行重建。编码端输入被破坏的文本，解码端输出重建的文本。BART探索了不同的破坏文本策略，最终提出通过随机打乱原始句子的顺序和用单个MASK符号进行区域遮罩的方式破坏文本。BART预处理和预训练示例：
```
python3 preprocess.py --corpus_path CLUECorpusSmall_bert_shuf_100w.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --seq_length 512 --target bart

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --config_path models/bart/base_config.json \
                    --output_model_path models/bart_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --learning_rate 1e-4 --batch_size 8 \
                    --span_masking --span_max_length 3 \
                    --embedding word_pos --tgt_embedding word_pos \
                    --encoder transformer --mask fully_visible --decoder transformer \
                    --target bart --tie_weights --has_lmtarget_bias
```

### XLM-RoBERTa
我们可以从Huggingface上下载多语言模型[XLM-RoBERTa-base](https://huggingface.co/xlm-roberta-base)和[XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large)并在其基础上做增量预训练。以base模型为例，首先将其转换为UER支持的格式：
```
python3 scripts/convert_xlmroberta_from_huggingface_to_uer.py --input_model_path models/xlmroberta_base_model_huggingface.bin \
                                                              --output_model_path models/xlmroberta_base_model_uer.bin \
                                                              --layers_num 12
```
由于原始的XLM-RoBERTa预训练权重使用了和BERT不同的特殊字符，因此我们需要在 *uer/utils/constants.py* 中修改特殊字符映射表路径，从 *models/special_tokens_map.json* 改为 *models/xlmroberta_special_tokens_map.json* 。
通过上面操作，我们就可以进行XLM-RoBERTa模型的预处理和增量预训练：
```
python3 preprocess.py --corpus_path corpora/CLUECorpusSmall_5000_lines.txt --spm_model_path models/xlmroberta_spm.model \
                      --dataset_path xlmroberta_zh_dataset.pt --seq_length 128 --processes_num 8 \
                      --dynamic_masking --tokenizer xlmroberta --target mlm

python3 pretrain.py --dataset_path xlmroberta_zh_dataset.pt --spm_model_path models/xlmroberta_spm.model \
                    --pretrained_model_path models/xlmroberta_base_model_uer.bin
                    --output_model_path models/output_model.bin --config_path models/xlm-roberta/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --batch_size 8 --tokenizer xlmroberta \
                    --total_steps 100000 --save_checkpoint_steps 10000 --report_steps 100 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
和常用的BERT和RoBERTa模型相比，原始的XLM-RoBERTa使用了不同的分词策略（*--tokenizer xlmroberta --spm_model_path models/xlmroberta_spm.model*）和不同的特殊字符映射表。

### Prefix LM
使用Prefix LM（在UniLM中被使用）作为目标任务进行预训练示例：
```
python3 preprocess.py --corpus_path corpora/csl_title_abstract.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --seq_length 256 --processes_num 8 --target prefixlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path output_model.bin --config_path models/bert/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --total_steps 5000 --save_checkpoint_steps 100 \
                    --embedding word_pos_seg --encoder transformer --mask causal_with_prefix --target prefixlm
```
其中 [*csl_title_abstract.txt*](https://share.weiyun.com/LwuQwWVl) 为科学文献标题摘要语料，标题和摘要用\t分隔，这也是 *--target prefixlm* 需要的语料格式。我们通过 *--mask causal_with_prefix* 指定前缀信息使用fully_visible方式遮罩，其余信息使用causal方式遮罩。需要注意的是，模型需要根据segment信息区分哪些部分是前缀。因此必须选择 *--embedding word_pos_seg* 。

### RealFormer
RealFormer在注意力矩阵上使用残差机制，让模型能够更快的收敛并且获得更好的效果。RealFormer预训练和预处理示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --dynamic_masking --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --has_residual_attention \
                    --target mlm
```
通过 *--has_residual_attention* 指定使用残差注意力机制。

### 更多组合
使用LSTM编码器和语言模型目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/rnn_config.json --learning_rate 1e-3 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 20000 --save_checkpoint_steps 5000 \
                    --embedding word --remove_embedding_layernorm --encoder lstm --target lm
```
我们使用*models/rnn_config.json*作为配置文件。

使用GRU编码器和语言模型目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/rnn_config.json --learning_rate 1e-3 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 20000 --save_checkpoint_steps 5000 \
                    --embedding word --remove_embedding_layernorm --encoder gru --target lm
```

使用GatedCNN编码器和语言模型目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/gatedcnn_9_config.json --learning_rate 1e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 20000 --save_checkpoint_steps 5000 \
                    --embedding word --remove_embedding_layernorm --encoder gatedcnn --target lm
```

使用机器翻译语料和任务进行预训练示例，目标任务和CoVe一样，但是使用Transformer作为编码器和解码器：
```
python3 preprocess.py --corpus_path corpora/iwslt_15_zh_en.tsv --vocab_path models/google_zh_vocab.txt \
                      --tgt_vocab_path models/google_uncased_en_vocab.txt \
                      --dataset_path dataset.pt --seq_length 64 --tgt_seq_length 64 \
                      --processes_num 8 --target seq2seq

python3 pretrain.py --dataset_path dataset.pt \
                    --vocab_path models/google_zh_vocab.txt --tgt_vocab_path models/google_uncased_en_vocab.txt \
                    --output_model_path output_model.bin --config_path models/encoder_decoder_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --report_steps 1000 --total_steps 50000 --save_checkpoint_steps 10000 \
                    --embedding word_sinusoidalpos --tgt_embedding word_sinusoidalpos \
                    --encoder transformer --mask fully_visible --decoder transformer \
                    --target seq2seq
```
其中 [*iwslt_15_zh_en.tsv*](https://share.weiyun.com/Jv8Hd5dM) 为中英平行语料，源语言和目标语言用\t分隔，这也是 *--target seq2seq* 需要的语料格式。我们可以使用预训练模型的编码器部分去初始化下游任务模型。

使用Transformer编码器和分类目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review_cls.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target cls

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/bert/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 2000 --save_checkpoint_steps 1000 --learning_rate 2e-5 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible \
                    --pooling first --target cls --labels_num 2
```
注意到我们需要通过 *--labels_num* 指定标签的个数。分类预训练语料格式如下（文本分类和文本对分类）：
```
1        instance1
0        instance2
1        instance3
```
```
1        instance1_text_a        instance1_text_b
0        instance2_text_a        instance1_text_b
1        instance3_text_a        instance1_text_b
```
\t作为分隔符。具体可见*corpora*文件夹中的*book_review_cls.txt*。

使用LSTM编码器和分类目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review_cls.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target cls

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/rnn_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --total_steps 2000 --save_checkpoint_steps 1000 --learning_rate 1e-3 \
                    --embedding word --remove_embedding_layernorm --encoder lstm \
                    --pooling max --target cls --labels_num 2
```

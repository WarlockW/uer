本项目支持多种分词（tokenization）方式。最常用的，也是项目默认使用的是BertTokenizer。BertTokenizer有两种使用方式：第一种是通过 *--vocab_path* 指定词典路径。然后使用BERT原始的tokenization策略，根据词典对句子进行切分；第二种是通过 *--spm_model_path* 指定sentencepiece模型路径，然后导入sentencepiece模块，加载sentencepiece模型，对句子进行切分。如果用户指定了 *--spm_model_path* ，那么使用sentencepiece进行tokeniztion。否则，用户必须指定 *--vocab_path* ，使用BERT原始的策略进行tokenization。

此外，本项目支持CharTokenizer和SpaceTokenizer。CharTokenizer将文本按照字符分开。如果文本全都是中文，则CharTokenizer和BertTokenizer等价。CharTokenizer逻辑简单，速度大于BertTokenizer。SpaceTokenizer将文本按照空格分开。可以事先对文本进行预处理（比如进行分词），将文本按照空格分开，然后使用SpaceTokenizer。对于CharTokenizer和SpaceTokenizer，如果用户指定了 *--spm_model_path* ，那么使用sentencepiece模型中的词典。否则，用户必须通过 *--vocab_path* 指定使用的词典。

本项目还支持XLMRobertaTokenizer（和原始实现一致）。XLMRobertaTokenizer使用sentencepiece模型对句子进行切分，通过 *--spm_model_path* 指定sentencepiece模型路径。此外，XLMRoBERTaTokenizer会对词典进行修改，加上特殊字符。由于XLMRobertaTokenizer使用了和默认情况不一致的特殊字符，需要按照下一段中提到的方法进行调整。

预处理、预训练、下游任务微调、推理阶段均需要通过 *--vocab_path* 或者 *--smp_model_path* 提供词典信息。如果用户使用自己的词典，默认情况下，填充字符、起始字符、分隔字符、遮罩字符、unknown字符分别为“[PAD]”、“[CLS]”、“[SEP]”、“[MASK]”、“[UNK]”。如果用户词典中的特殊字符和默认的不一致，需要提供特殊字符映射表，比如 *models/xlmroberta_special_tokens_map.json* ，然后修改 *uer/utils/constants.py* 中的特殊字符映射表路径。

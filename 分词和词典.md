本项目支持多种分词（tokenization）方式。最常用的，也是项目默认使用的是BertTokenizer。BertTokenizer有两种使用方式：第一种是通过 *--vocab_path* 指定词典路径。然后使用BERT原始的tokenization策略，根据词典对句子进行切分；第二种是通过 *--spm_model_path* 指定sentencepiece模型路径，然后导入sentencepiece模块，加载sentencepiece模型，对句子进行切分。如果用户指定了 *--spm_model_path* ，那么使用sentencepiece进行tokeniztion。否则，用户必须指定 *--vocab_path* ，使用BERT原始的策略进行tokenization。

此外，本项目还支持CharTokenizer和SpaceTokenizer。CharTokenizer将文本按照字符分开。如果文本全都是中文，则CharTokenizer和BertTokenizer等价。CharTokenizer逻辑简单，速度大于BertTokenizer。SpaceTokenizer将文本按照空格分开。可以事先对文本进行预处理（比如进行分词），将文本按照空格分开，然后使用SpaceTokenizer。对于CharTokenizer和SpaceTokenizer，如果用户指定了 *--spm_model_path* ，那么使用sentencepiece模型中的词典。否则，用户必须通过 *--vocab_path* 指定使用的词典。

预处理、预训练、下游任务微调阶段均需要通过 *--vocab_path* 或者 *--smp_model_path* 提供词典信息。如果用户使用自己的词典，需要保证以下几点：1）填充字符PAD的ID为0；2）起始字符、分隔字符、遮罩字符分别为“[CLS]”、“[SEP]”、“[MASK]”；3)如果指定 *--vocab_path* ，则unknown字符为“[UNK]”。如果指定 *--spm_model_path* ，则unknown字符为”\<unk\>“。


[**English**](https://github.com/dbiir/UER-py/wiki/Instructions) | [**中文**](https://github.com/dbiir/UER-py/wiki/使用说明)

UER-py supports a wide range of pre-training models and downstream tasks. This section shows the comprehensive use-cases when using the UER-py. In many cases, we use BERT model to demonstrate how to use UER-py by default.


- #### [Preprocess the data](https://github.com/dbiir/UER-py/wiki/Preprocess-the-data)


- #### [Pretrain the model](https://github.com/dbiir/UER-py/wiki/Pretrain-the-model)


- #### [Pretrain models with different encoders and targets](https://github.com/dbiir/UER-py/wiki/Pretrain-models-with-different-encoders-and-targets)


- #### [Finetune on downstream tasks](https://github.com/dbiir/UER-py/wiki/Finetune-on-downstream-tasks)


- #### [Tokenization and vocabulary](https://github.com/dbiir/UER-py/wiki/Tokenization-and-vocabulary)


- #### [Scripts](https://github.com/dbiir/UER-py/wiki/预训练相关功能脚本)

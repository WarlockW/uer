[**English**](https://github.com/dbiir/UER-py/wiki/Instructions) | [**中文**](https://github.com/dbiir/UER-py/wiki/使用说明)

UER-py supports a wide range of pre-training models and downstream tasks. This section shows the comprehensive use-cases when using the UER-py. In many cases, we use BERT model to demonstrate how to use UER-py by default.


- #### [Preprocess the data](https://github.com/dbiir/UER-py/wiki/Preprocess-the-data)


- #### [Pretrain the model](https://github.com/dbiir/UER-py/wiki/Pretrain-the-model)


- #### [Pretrain models with different encoders and targets](https://github.com/dbiir/UER-py/wiki/Pretrain-models-with-different-encoders-and-targets)


- #### [Finetune on downstream tasks](https://github.com/dbiir/UER-py/wiki/Finetune-on-downstream-tasks)


- #### [Tokenization and vocabulary](https://github.com/dbiir/UER-py/wiki/Tokenization-and-vocabulary)


- #### [Scripts](https://github.com/dbiir/UER-py/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9B%B8%E5%85%B3%E5%8A%9F%E8%83%BD%E8%84%9A%E6%9C%AC)


UER-py supports a wide range of pre-training models and downstream tasks. This section shows the comprehensive use-cases when using the UER-py. In many cases, we use BERT model to demonstrate how to use UER-py by default.


- #### [Preprocess the data](https://github.com/dbiir/UER-py/wiki/Preprocess-the-data)


- #### [Pretrain the model](https://github.com/dbiir/UER-py/wiki/Pretrain-the-model)


- #### [Pretraining model examples](https://github.com/dbiir/UER-py/wiki/Pretraining-model-examples)


- #### [Finetune the tasks](https://github.com/dbiir/UER-py/wiki/Finetune-the-tasks)


- #### [Tokenization and vocabulary](https://github.com/dbiir/UER-py/wiki/Tokenization-and-vocabulary)


- #### [Scripts](https://github.com/dbiir/UER-py/wiki/Scripts)

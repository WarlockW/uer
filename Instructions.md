[**English**](https://github.com/dbiir/UER-py/wiki/Instructions) | [**中文**](https://github.com/dbiir/UER-py/wiki/使用说明)

UER-py supports a wide range of pre-training models and downstream tasks. This section shows the comprehensive use-cases when using the UER-py. In many cases, we use BERT model to demonstrate how to use UER-py by default.


- #### [Preprocess the data](https://github.com/dbiir/UER-py/wiki/Preprocess-the-data)


- #### [Pretrain the model](https://github.com/dbiir/UER-py/wiki/Pretrain-the-model)


- #### [More pretraining models](https://github.com/dbiir/UER-py/wiki/More-pretraining-models)


- #### [Finetune the tasks](https://github.com/dbiir/UER-py/wiki/Finetune-the-tasks)


- #### [Tokenization and vocabulary](https://github.com/dbiir/UER-py/wiki/Tokenization-and-vocabulary)


- #### [Scripts](https://github.com/dbiir/UER-py/wiki/预训练相关功能脚本)

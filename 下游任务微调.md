UER支持多种下游任务，包括文本分类（classification）、文本对分类（pair classification）、基于文档的问答（document-based question answering）、序列标注（sequence labeling）、机器阅读理解（machine reading comprehension）等。注意下游任务使用的词向量层和编码器层需要与预训练模型相匹配，配置文件也需要保持一致。本章节用到的预训练模型可以在[预训练模型仓库](https://github.com/dbiir/UER-py/wiki/预训练模型仓库)中下载

## 分类
run_classifier.py 在编码器上接两层前向神经网络。
```
usage: run_classifier.py [-h] [--pretrained_model_path PRETRAINED_MODEL_PATH]
                         [--output_model_path OUTPUT_MODEL_PATH]
                         [--vocab_path VOCAB_PATH]
                         [--spm_model_path SPM_MODEL_PATH] --train_path
                         TRAIN_PATH --dev_path DEV_PATH
                         [--test_path TEST_PATH] [--config_path CONFIG_PATH]
                         [--embedding {word,word_pos,word_pos_seg,word_sinusoidalpos}]
                         [--max_seq_length MAX_SEQ_LENGTH]
                         [--relative_position_embedding]
                         [--relative_attention_buckets_num RELATIVE_ATTENTION_BUCKETS_NUM]
                         [--remove_embedding_layernorm]
                         [--remove_attention_scale]
                         [--encoder {transformer,rnn,lstm,gru,birnn,bilstm,bigru,gatedcnn}]
                         [--mask {fully_visible,causal,causal_with_prefix}]
                         [--layernorm_positioning {pre,post}]
                         [--feed_forward {dense,gated}]
                         [--remove_transformer_bias] [--layernorm {normal,t5}]
                         [--bidirectional]
                         [--factorized_embedding_parameterization]
                         [--parameter_sharing] [--learning_rate LEARNING_RATE]
                         [--warmup WARMUP] [--fp16]
                         [--fp16_opt_level {O0,O1,O2,O3}]
                         [--optimizer {adamw,adafactor}]
                         [--scheduler {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                         [--batch_size BATCH_SIZE] [--seq_length SEQ_LENGTH]
                         [--dropout DROPOUT] [--epochs_num EPOCHS_NUM]
                         [--report_steps REPORT_STEPS] [--seed SEED]
                         [--pooling {mean,max,first,last}]
                         [--tokenizer {bert,char,space}] [--soft_targets]
                         [--soft_alpha SOFT_ALPHA]
```
*run_classifier.py*文本分类使用示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --epochs_num 3 --batch_size 64 \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
默认使用CLS位置向量进行预测（*--pooling first*）。

*run_classifier.py* 文本对分类使用示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                   --train_path datasets/lcqmc/train.tsv \
                                   --dev_path datasets/lcqmc/dev.tsv \
                                   --test_path datasets/lcqmc/test.tsv \
                                   --epochs_num 3 --batch_size 64 \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
可以从下游任务数据集章节中下载LCQMC数据集并把它放到 *datasets* 文件夹下。

*inference/run_classifier_infer.py*文本分类推理（预测）使用示例：
```
python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin --vocab_path models/google_zh_vocab.txt \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv \
                                          --labels_num 2 --seq_length 128 --output_logits --output_prob \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```
对于分类数据集，模型会对text_a列的文本进行预测。对于文本对分类数据集，模型会对text_a列和text_b列的文本进行预测。<br>
*--labels_num* 指定分类任务标签的个数。由于待预测文件中不包括标签，因此需要给出标签个数的信息。 <br>
*--output_logits* 指定模型预测的logits，列名为logits。<br>
*--output_prob* 指定模型预测的概率，列名为prob。<br>
*--seq_length* 指定序列长度，推荐和训练时保持一致。

注意到BERT模型和RoBERTa模型的网络结构相同，因此加载BERT和RoBERTa的方式没有区别。

使用ALBERT模型进行分类任务示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/google_zh_albert_base_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/albert/base_config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --learning_rate 4e-5 \
                                   --epochs_num 5 --batch_size 32 \
                                   --factorized_embedding_parameterization --parameter_sharing \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
ALBERT的效果对学习率、训练轮数等超参数比较敏感，有时需要多次尝试。在ALBERT预训练阶段，dropout是被关掉的（参见 *models/albert/base_config.json*）。推荐在微调阶段将配置文件中的dropout设置为0.1。 <br>
使用在下游任务上微调后的ALBERT模型进行预测：
```
python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/albert/base_config.json \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv --labels_num 2 \
                                          --factorized_embedding_parameterization --parameter_sharing \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```

使用GPT-2预训练模型进行分类任务微调和推理示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/cluecorpussmall_gpt2_seq1024_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/gpt2/config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --epochs_num 3 --batch_size 32 \
                                   --embedding word_pos --remove_embedding_layernorm \
                                   --encoder transformer --mask causal --layernorm_positioning pre --pooling mean

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/gpt2/config.json \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv --labels_num 2 \
                                          --embedding word_pos --remove_embedding_layernorm \
                                          --encoder transformer --mask causal --layernorm_positioning pre --pooling mean
```
这里我们使用了 *--pooling mean* 。同样我们还可以使用 *--pooling max* 和 *--pooling last* 。但是由于这里使用了语言模型（*--mask causal*），*--pooling first* （CLS位置向量）是不能使用的。

使用LSTM预训练模型进行分类任务微调和推理示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/cluecorpussmall_lstm_lm_model.bin \
                                   --vocab_path models/google_zh_vocab.txt --config_path models/rnn_config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --learning_rate 1e-3 --batch_size 64 --epochs_num 5 \
                                   --embedding word --remove_embedding_layernorm --encoder lstm --pooling mean

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/rnn_config.json \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv \
                                          --labels_num 2 \
                                          --embedding word --remove_embedding_layernorm --encoder lstm --pooling mean
```

使用ELMo预训练模型进行分类任务微调和推理示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/chnsenticorp_elmo_model.bin \
                                   --vocab_path models/google_zh_vocab.txt --config_path models/birnn_config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --epochs_num 5  --batch_size 64 --seq_length 192 --learning_rate 5e-4 \
                                   --embedding word --remove_embedding_layernorm --encoder bilstm --pooling max

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/birnn_config.json \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv \
                                          --seq_length 192 --labels_num 2 \
                                          --embedding word --remove_embedding_layernorm --encoder bilstm --pooling max
```

使用GatedCNN预训练模型进行分类任务微调和推理示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/cluecorpussmall_gatedcnn_lm_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/gatedcnn_9_config.json \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --epochs_num 5  --batch_size 64 --learning_rate 5e-5 \
                                   --embedding word --remove_embedding_layernorm --encoder gatedcnn --pooling mean

python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/gatedcnn_9_config.json \
                                          --test_path datasets/douban_book_review/test_nolabel.tsv \
                                          --prediction_path datasets/douban_book_review/prediction.tsv \
                                          --labels_num 2 \
                                          --embedding word --remove_embedding_layernorm --encoder gatedcnn --pooling mean
```

UER-py 支持对分类任务进行多任务学习，模型共享词向量层和编码层。<br>
同时对两个情感分类数据集进行微调示例：
```
python3 finetune/run_classifier_mt.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                      --dataset_path_list datasets/douban_book_review/ datasets/chnsenticorp/ \
                                      --epochs_num 1 --batch_size 64 \
                                      --embedding word_pos_seg --encoder transformer --mask fully_visible
```
*--dataset_path_list* 指定多任务数据集文件夹路径。每个文件夹需要包括训练集 *train.tsv* 和验证集 *dev.tsv* 。

UER-py 支持对分类任务进行网格搜索：
```
python3 finetune/run_classifier_grid.py --vocab_path models/google_zh_vocab.txt \
                                        --config_path models/bert/tiny_config.json \
                                        --train_path datasets/douban_book_review/train.tsv \
                                        --dev_path datasets/douban_book_review/dev.tsv \
                                        --embedding word_pos_seg --encoder transformer --mask fully_visible \
                                        --batch_size_list 32 64 --learning_rate_list 3e-5 1e-4 3e-4 --epochs_num_list 3 5 8
```
这里对不同的大小的 batch size ，学习率，训练轮数进行网格搜索。

UER-py 支持对分类任务进行蒸馏。<br>
首先训练teacher模型。这里使用24层的BERT-large模型。可在本项目中下载中文BERT-large预训练模型并在其基础上进行训练，使用示例：
```
python3 finetune/run_classifier.py --pretrained_model_path models/mixed_corpus_bert_large_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/bert_large_config.json \
                                   --output_model_path models/teacher_classifier_model.bin \
                                   --train_path datasets/douban_book_review/train.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --epochs_num 3 --batch_size 32 \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
然后使用teacher模型进行预测，生成伪标签以及logits：
```
python3 inference/run_classifier_infer.py --load_model_path models/teacher_classifier_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/bert_large_config.json --test_path text.tsv \
                                          --prediction_path label_logits.tsv --labels_num 2 --output_logits \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```
输入文件 *text.tsv* 包括文本，即包括text_a列（对于文本对分类，则包括text_a和text_b列）。*text.tsv* 可以是下游任务数据集的训练集/验证集/测试集，比如，可以把 *datasets/douban_book_review/train.tsv* 作为输入（ *--test_path* ）。但更推荐使用和下游任务数据集相似的大规模外部数据，比如对于情感分类数据集，可以从网络中爬取大规模的评论文本。更多的数据通常能为蒸馏带来更好的效果。<br>
Teacher模型输出的预测结果 *label_logits.tsv* 包括label列和logits列。<br>
然后将预测结果 *label_logits.tsv* 和原始文本 *text.tsv* 合并为 *text_label_logits.tsv* 。这个文件包括text_a列（对于文本对分类，则包括text_a和text_b列）、label列（硬标签）、和logits列（软标签）。<br>
这里，student模型为3层的BERT-tiny模型。可在本项目中下载BERT-tiny预训练模型并在其基础上进行训练。<br>
我们让BERT-tiny模型同时学习BERT-large模型预测的硬标签和软标签：
```
python3 finetune/run_classifier.py --pretrained_model_path mixed_corpus_bert_tiny_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/bert_tiny_config.json \
                                   --train_path text_label_logits.tsv \
                                   --dev_path datasets/douban_book_review/dev.tsv \
                                   --test_path datasets/douban_book_review/test.tsv \
                                   --epochs_num 3 --batch_size 64 --soft_targets --soft_alpha 0.5 \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
*--soft_targets* 指定模型读取logits列进行训练，这里使用mean-squared-error(MSE)损失函数。<br>
*--soft_alpha* 指定软标签loss的权重。训练的loss为硬标签损失cross-entropy(CE)和软标签损失mean-squared-error(MSE)的加权平均。

## 基于文档的问答
*run_dbqa.py* 使用和 *run_classifier.py* 相同的网络结构。
```
usage: run_dbqa.py [-h] [--pretrained_model_path PRETRAINED_MODEL_PATH]
                   [--output_model_path OUTPUT_MODEL_PATH]
                   [--vocab_path VOCAB_PATH] [--spm_model_path SPM_MODEL_PATH]
                   --train_path TRAIN_PATH --dev_path DEV_PATH
                   [--test_path TEST_PATH] [--config_path CONFIG_PATH]
                   [--embedding {word,word_pos,word_pos_seg,word_sinusoidalpos}]
                   [--max_seq_length MAX_SEQ_LENGTH]
                   [--relative_position_embedding]
                   [--relative_attention_buckets_num RELATIVE_ATTENTION_BUCKETS_NUM]
                   [--remove_embedding_layernorm] [--remove_attention_scale]
                   [--encoder {transformer,rnn,lstm,gru,birnn,bilstm,bigru,gatedcnn}]
                   [--mask {fully_visible,causal,causal_with_prefix}]
                   [--layernorm_positioning {pre,post}]
                   [--feed_forward {dense,gated}] [--remove_transformer_bias]
                   [--layernorm {normal,t5}] [--bidirectional]
                   [--factorized_embedding_parameterization]
                   [--parameter_sharing] [--learning_rate LEARNING_RATE]
                   [--warmup WARMUP] [--fp16] [--fp16_opt_level {O0,O1,O2,O3}]
                   [--optimizer {adamw,adafactor}]
                   [--scheduler {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                   [--batch_size BATCH_SIZE] [--seq_length SEQ_LENGTH]
                   [--dropout DROPOUT] [--epochs_num EPOCHS_NUM]
                   [--report_steps REPORT_STEPS] [--seed SEED]
                   [--pooling {mean,max,first,last}]
                   [--tokenizer {bert,char,space}] [--soft_targets]
                   [--soft_alpha SOFT_ALPHA]
```
基于文档的问答任务可以转换为文本对分类任务，text_a列为问题，text_b列包含可能存在答案的句子。<br>
*run_dbqa.py* 使用示例：
```
python3 finetune/run_dbqa.py --pretrained_model_path models/google_zh_model.bin \
                             --vocab_path models/google_zh_vocab.txt \
                             --train_path datasets/nlpcc-dbqa/train.tsv \
                             --dev_path datasets/nlpcc-dbqa/dev.tsv \
                             --test datasets/nlpcc-dbqa/test.tsv \
                             --epochs_num 3 --batch_size 64 \
                             --embedding word_pos_seg --encoder transformer --mask fully_visible
```
*inference/run_classifier_infer.py* 基于文档的问答任务预测使用示例:
```
python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --test_path datasets/nlpcc-dbqa/test_nolabel.tsv \
                                          --prediction_path datasets/nlpcc-dbqa/prediction.tsv --labels_num 2 \
                                          --output_logits --output_prob \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```
使用ALBERT模型进行基于文档的问答任务示例：
```
python3 finetune/run_dbqa.py --pretrained_model_path models/google_zh_albert_base_model.bin \
                             --vocab_path models/google_zh_vocab.txt \
                             --config_path models/albert/base_config.json \
                             --train_path datasets/nlpcc-dbqa/train.tsv \
                             --dev_path datasets/nlpcc-dbqa/dev.tsv \
                             --test datasets/nlpcc-dbqa/test.tsv \
                             --epochs_num 3 --batch_size 64 \
                             --factorized_embedding_parameterization --parameter_sharing \
                             --embedding word_pos_seg --encoder transformer --mask fully_visible
```
使用微调后的ALBERT模型进行预测示例：
```
python3 inference/run_classifier_infer.py --load_model_path models/finetuned_model.bin \
                                          --vocab_path models/google_zh_vocab.txt \
                                          --config_path models/albert/base_config.json \
                                          --test_path datasets/nlpcc-dbqa/test_nolabel.tsv \
                                          --prediction_path datasets/nlpcc-dbqa/prediction.tsv --labels_num 2 \
                                          --factorized_embedding_parameterization --parameter_sharing \
                                          --embedding word_pos_seg --encoder transformer --mask fully_visible
```

## 序列标注
*run_ner.py* 在编码器上接一层前向神经网络。
```
usage: run_ner.py [-h] [--pretrained_model_path PRETRAINED_MODEL_PATH]
                  [--output_model_path OUTPUT_MODEL_PATH]
                  [--vocab_path VOCAB_PATH] [--spm_model_path SPM_MODEL_PATH]
                  --train_path TRAIN_PATH --dev_path DEV_PATH
                  [--test_path TEST_PATH] [--config_path CONFIG_PATH]
                  [--embedding {word,word_pos,word_pos_seg,word_sinusoidalpos}]
                  [--max_seq_length MAX_SEQ_LENGTH]
                  [--relative_position_embedding]
                  [--relative_attention_buckets_num RELATIVE_ATTENTION_BUCKETS_NUM]
                  [--remove_embedding_layernorm] [--remove_attention_scale]
                  [--encoder {transformer,rnn,lstm,gru,birnn,bilstm,bigru,gatedcnn}]
                  [--mask {fully_visible,causal,causal_with_prefix}]
                  [--layernorm_positioning {pre,post}]
                  [--feed_forward {dense,gated}] [--remove_transformer_bias]
                  [--layernorm {normal,t5}] [--bidirectional]
                  [--factorized_embedding_parameterization]
                  [--parameter_sharing] [--learning_rate LEARNING_RATE]
                  [--warmup WARMUP] [--fp16] [--fp16_opt_level {O0,O1,O2,O3}]
                  [--optimizer {adamw,adafactor}]
                  [--scheduler {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                  [--batch_size BATCH_SIZE] [--seq_length SEQ_LENGTH]
                  [--dropout DROPOUT] [--epochs_num EPOCHS_NUM]
                  [--report_steps REPORT_STEPS] [--seed SEED] --label2id_path
                  LABEL2ID_PATH
```
*run_ner.py* 序列标注使用示例：
```
python3 finetune/run_ner.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                            --train_path datasets/msra_ner/train.tsv \
                            --dev_path datasets/msra_ner/dev.tsv \
                            --test_path datasets/msra_ner/test.tsv \
                            --label2id_path datasets/msra_ner/label2id.json \
                            --epochs_num 5 --batch_size 16 \
                            --embedding word_pos_seg --encoder transformer --mask fully_visible
```
预测示例：
```
python3 inference/run_ner_infer.py --load_model_path models/finetuned_model.bin --vocab_path models/google_zh_vocab.txt \
                                   --test_path datasets/msra_ner/test_nolabel.tsv \
                                   --prediction_path datasets/msra_ner/prediction.tsv \
                                   --label2id_path datasets/msra_ner/label2id.json \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```
使用ALBERT模型进行序列标注示例：
```
python3 finetune/run_ner.py --pretrained_model_path models/google_zh_albert_base_model.bin \
                            --vocab_path models/google_zh_vocab.txt \
                            --config_path models/albert/base_config.json \
                            --train_path datasets/msra_ner/train.tsv \
                            --dev_path datasets/msra_ner/dev.tsv \
                            --test_path datasets/msra_ner/test.tsv \
                            --label2id_path datasets/msra_ner/label2id.json \
                            --epochs_num 5 --batch_size 16 --learning_rate 1e-4 \
                            --factorized_embedding_parameterization --parameter_sharing \
                            --embedding word_pos_seg --encoder transformer --mask fully_visible

python3 inference/run_ner_infer.py --load_model_path models/finetuned_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/albert/base_config.json \
                                   --test_path datasets/msra_ner/test_nolabel.tsv \
                                   --prediction_path datasets/msra_ner/prediction.tsv \
                                   --label2id_path datasets/msra_ner/label2id.json \
                                   --factorized_embedding_parameterization --parameter_sharing \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```

使用ELMo模型进行序列标注示例：
```
python3 finetune/run_ner.py --pretrained_model_path models/cluecorpussmall_elmo_model.bin-500000 \
                            --vocab_path models/google_zh_vocab.txt \
                            --config_path models/birnn_config.json \
                            --train_path datasets/msra_ner/train.tsv \
                            --dev_path datasets/msra_ner/dev.tsv \
                            --test_path datasets/msra_ner/test.tsv \
                            --label2id_path datasets/msra_ner/label2id.json \
                            --epochs_num 5  --batch_size 16  --learning_rate 5e-4 \
                            --embedding word --remove_embedding_layernorm --encoder bilstm

python3 inference/run_ner_infer.py --load_model_path models/finetuned_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/birnn_config.json \
                                   --test_path datasets/msra_ner/test_nolabel.tsv \
                                   --prediction_path datasets/msra_ner/prediction.tsv \
                                   --label2id_path datasets/msra_ner/label2id.json \
                                   --embedding word --remove_embedding_layernorm --encoder bilstm
```

## 机器阅读理解
*run_cmrc.py* 在编码器上接一层前向神经网络。
```
usage: run_cmrc.py [-h] [--pretrained_model_path PRETRAINED_MODEL_PATH]
                   [--output_model_path OUTPUT_MODEL_PATH]
                   [--vocab_path VOCAB_PATH] [--spm_model_path SPM_MODEL_PATH]
                   --train_path TRAIN_PATH --dev_path DEV_PATH
                   [--test_path TEST_PATH] [--config_path CONFIG_PATH]
                   [--embedding {word,word_pos,word_pos_seg,word_sinusoidalpos}]
                   [--max_seq_length MAX_SEQ_LENGTH]
                   [--relative_position_embedding]
                   [--relative_attention_buckets_num RELATIVE_ATTENTION_BUCKETS_NUM]
                   [--remove_embedding_layernorm] [--remove_attention_scale]
                   [--encoder {transformer,rnn,lstm,gru,birnn,bilstm,bigru,gatedcnn}]
                   [--mask {fully_visible,causal,causal_with_prefix}]
                   [--layernorm_positioning {pre,post}]
                   [--feed_forward {dense,gated}] [--remove_transformer_bias]
                   [--layernorm {normal,t5}] [--bidirectional]
                   [--factorized_embedding_parameterization]
                   [--parameter_sharing] [--learning_rate LEARNING_RATE]
                   [--warmup WARMUP] [--fp16] [--fp16_opt_level {O0,O1,O2,O3}]
                   [--optimizer {adamw,adafactor}]
                   [--scheduler {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                   [--batch_size BATCH_SIZE] [--seq_length SEQ_LENGTH]
                   [--dropout DROPOUT] [--epochs_num EPOCHS_NUM]
                   [--report_steps REPORT_STEPS] [--seed SEED]
                   [--doc_stride DOC_STRIDE]
```
*run_cmrc.py* 抽取式阅读理解使用示例：
```
python3 finetune/run_cmrc.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                             --train_path datasets/cmrc2018/train.json --dev_path datasets/cmrc2018/dev.json \
                             --epochs_num 2 --batch_size 8 --seq_length 512 \
                             --embedding word_pos_seg --encoder transformer --mask fully_visible
```
其中 *train.json* 和 *dev.json* 文件是squad类型格式。CMRC2018数据集的测试集答案没有公开，因此这里没有指定 *--test_path*。

预测示例：
```
python3  inference/run_cmrc_infer.py --load_model_path models/finetuned_model.bin \
                                     --vocab_path models/google_zh_vocab.txt \
                                     --test_path datasets/cmrc2018/test.json \
                                     --prediction_path datasets/cmrc2018/prediction.json --seq_length 512 \
                                     --embedding word_pos_seg --encoder transformer --mask fully_visible
```
使用ALBERT-xxlarge模型进行抽取式阅读理解示例：
```
python3 finetune/run_cmrc.py --pretrained_model_path models/google_zh_albert_xxlarge_model.bin \
                             --vocab_path models/google_zh_vocab.txt \
                             --config_path models/albert/xxlarge_config.json \
                             --train_path datasets/cmrc2018/train.json --dev_path datasets/cmrc2018/dev.json \
                             --epochs_num 2 --batch_size 8 --seq_length 512 --learning_rate 1e-5 \
                             --factorized_embedding_parameterization --parameter_sharing \
                             --embedding word_pos_seg --encoder transformer --mask fully_visible
```
使用微调后的ALBERT模型进行预测示例：
```
python3 inference/run_cmrc_infer.py --load_model_path models/finetuned_model.bin \
                                     --vocab_path models/google_zh_vocab.txt \
                                     --config_path models/albert/xxlarge_config.json \
                                     --test_path datasets/cmrc2018/test.json \
                                     --prediction_path datasets/cmrc2018/prediction.json --seq_length 512 \
                                     --factorized_embedding_parameterization --parameter_sharing \
                                     --embedding word_pos_seg --encoder transformer --mask fully_visible
```

## 多选式阅读理解
C3数据集是一个多选式阅读理解数据集。给定上下文和问题，需要从4个候选答案中进行选择。*run_c3.py* 在编码器上接一层前向神经网络。
```
usage: run_c3.py [-h] [--pretrained_model_path PRETRAINED_MODEL_PATH]
                 [--output_model_path OUTPUT_MODEL_PATH]
                 [--vocab_path VOCAB_PATH] [--spm_model_path SPM_MODEL_PATH]
                 --train_path TRAIN_PATH --dev_path DEV_PATH
                 [--test_path TEST_PATH] [--config_path CONFIG_PATH]
                 [--embedding {word,word_pos,word_pos_seg,word_sinusoidalpos}]
                 [--max_seq_length MAX_SEQ_LENGTH]
                 [--relative_position_embedding]
                 [--relative_attention_buckets_num RELATIVE_ATTENTION_BUCKETS_NUM]
                 [--remove_embedding_layernorm] [--remove_attention_scale]
                 [--encoder {transformer,rnn,lstm,gru,birnn,bilstm,bigru,gatedcnn}]
                 [--mask {fully_visible,causal,causal_with_prefix}]
                 [--layernorm_positioning {pre,post}]
                 [--feed_forward {dense,gated}] [--remove_transformer_bias]
                 [--layernorm {normal,t5}] [--bidirectional]
                 [--factorized_embedding_parameterization]
                 [--parameter_sharing] [--learning_rate LEARNING_RATE]
                 [--warmup WARMUP] [--fp16] [--fp16_opt_level {O0,O1,O2,O3}]
                 [--optimizer {adamw,adafactor}]
                 [--scheduler {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                 [--batch_size BATCH_SIZE] [--seq_length SEQ_LENGTH]
                 [--dropout DROPOUT] [--epochs_num EPOCHS_NUM]
                 [--report_steps REPORT_STEPS] [--seed SEED]
                 [--max_choices_num MAX_CHOICES_
```
多选式阅读理解使用示例：
```
python3 finetune/run_c3.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                           --train_path datasets/c3/train.json --dev_path datasets/c3/dev.json \
                           --epochs_num 8 --batch_size 8 --seq_length 512 --max_choices_num 4 \
                           --embedding word_pos_seg --encoder transformer --mask fully_visible
```
C3数据集的测试集答案没有公开，因此这里没有指定 *--test_path* 。<br>
模型实际的batch size大小为 *--batch_size* 乘以 *--max_choices_num* 。 <br>
C3数据集的阅读理解题，最多包括4个选项，因此 *--max_choices_num* 设置为4。<br>
预测示例：
```
python3 inference/run_c3_infer.py --load_model_path models/finetuned_model.bin \
                                  --vocab_path models/google_zh_vocab.txt \
                                  --test_path datasets/c3/test.json \
                                  --prediction_path datasets/c3/prediction.json \
                                  --max_choices_num 4 --seq_length 512 \
                                  --embedding word_pos_seg --encoder transformer --mask fully_visible
```
使用ALBERT-xlarge模型进行多选式阅读理解示例：
```
python3 finetune/run_c3.py --pretrained_model_path models/google_zh_albert_xlarge_model.bin \
                           --vocab_path models/google_zh_vocab.txt \
                           --config_path models/albert/xlarge_config.json \
                           --train_path datasets/c3/train.json --dev_path datasets/c3/dev.json \
                           --epochs_num 8 --batch_size 8 --seq_length 512 --max_choices_num 4 \
                           --factorized_embedding_parameterization --parameter_sharing \
                           --embedding word_pos_seg --encoder transformer --mask fully_visible
```

使用微调后的ALBERT模型进行预测示例：
```
python3  inference/run_c3_infer.py --load_model_path models/finetuned_model.bin \
                                   --vocab_path models/google_zh_vocab.txt \
                                   --config_path models/albert/xlarge_config.json \
                                   --test_path datasets/c3/test.json \
                                   --prediction_path datasets/c3/prediction.json \
                                   --max_choices_num 4 --seq_length 512 \
                                   --factorized_embedding_parameterization --parameter_sharing \
                                   --embedding word_pos_seg --encoder transformer --mask fully_visible
```

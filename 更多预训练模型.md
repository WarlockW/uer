在预处理和预训练过程中，可以通过选择不同的模块进行组合（比如词向量模块、编码器模块、预训练目标模块）得到不同的预训练模型。下面给出了相关示例。


#### RoBERTa
RoBERTa预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --dynamic_masking --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
RoBERTa使用动态遮罩策略，mlm目标任务，并且允许样本包括跨文档的内容。 <br>
如果预训练的文档比较短，则推荐不使用 *--full_sentences* 选项。 <br>
注意到RoBERTa去掉了句子预测任务，因此输入的语料格式是一行一个文档，与BERT要求的语料格式不同。在上面的示例中，预处理加载的语料是*corpora/book_review.txt*，而不是*corpora/book_review_bert.txt*。 <br>
RoBERTa可以加载已有的BERT模型进行增量预训练（反之亦然）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --dynamic_masking --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --pretrained_model_path models/google_zh_model.bin \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 2e-5 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```


#### ALBERT
ALBERT预处理和预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target albert

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --config_path models/albert/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --factorized_embedding_parameterization --parameter_sharing \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target albert
```
ALBERT使用的语料的格式和BERT一样。 <br>
*--target albert* 表示使用ALBERT的目标任务，包含了遮罩语言模型和句子顺序预测任务。<br>
*--factorized_embedding_parameterization* 表示词向量层分解。<br>
*--parameter_sharing* 表示编码层参数共享。<br>
我们提供了4种ALBERT模型的配置文件，并放在 *models/albert/* 文件夹下，分别是 base_config.json , large_config.json , xlarge_config.json , xxlarge_config.json 。<br>
加载Google中文ALBERT预训练模型进行增量预训练示例（从预训练模型仓库章节可以下载转换好的权重）：
```
python3 preprocess.py --corpus_path corpora/book_review_bert.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target albert

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --pretrained_model_path models/google_zh_albert_base_model.bin \
                    --output_model_path models/output_model.bin \
                    --config_path models/albert/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 2e-5 \
                    --factorized_embedding_parameterization --parameter_sharing \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target albert

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --pretrained_model_path models/google_zh_albert_xlarge_model.bin \
                    --output_model_path models/output_model.bin \
                    --config_path models/albert/xlarge_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 2e-5 \
                    --factorized_embedding_parameterization --parameter_sharing \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target albert
```


#### SpanBERT
SpanBERT除了引入区域遮罩，还引入了span boundary目标任务。这里我们只考虑区域遮罩。<br>
SpanBERT预处理和预训练示例（静态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --dup_factor 20 \
                      --span_masking --span_geo_prob 0.3 --span_max_length 5 --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
*--dup_factor* 指定语料复制的次数（每次使用不同的遮罩），默认复制5次。<br>
SpanBERT预处理和预训练示例（动态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 \
                      --dynamic_masking --target mlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --span_masking --span_geo_prob 0.3 --span_max_length 5 \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```


#### BERT-WWM
BERT-WWM对整词进行遮罩。这里我们使用MLM预训练目标任务。
BERT-WWM预处理和预训练示例（静态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt \
                      --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt \
                      --processes_num 8 --dup_factor 20 \
                      --whole_word_masking \
                      --target mlm

python3 pretrain.py --dataset_path dataset.pt \
                    --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```
*--whole_word_masking* 指定使用整词遮罩。
BERT-WWM预处理和预训练示例（动态遮罩）：
```
python3 preprocess.py --corpus_path corpora/book_review.txt \
                      --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt \
                      --processes_num 8 --dynamic_masking \
                      --target mlm

python3 pretrain.py --dataset_path dataset.pt \
                    --vocab_path models/google_zh_vocab.txt \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7  --learning_rate 1e-4 \
                    --whole_word_masking \
                    --total_steps 10000 --save_checkpoint 5000 \
                    --embedding word_pos_seg --encoder transformer --mask fully_visible --target mlm
```


#### GPT
GPT预处理和预训练示例:
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --output_model_path models/output_model.bin \
                    --config_path models/gpt2/config.json --learning_rate 1e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --embedding word_pos --encoder transformer --mask causal --target lm
```
GPT使用的语料的格式和RoBERTa一样。我们可以通过指定 *--embedding word_pos --encoder transformer --mask causal --target lm* 复现预训练GPT模型。


#### GPT-2
GPT-2预处理和预训练示例:
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --output_model_path models/output_model.bin \
                    --config_path models/gpt2/config.json --learning_rate 1e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --embedding word_pos --remove_embedding_layernorm \
                    --encoder transformer --mask causal --layernorm_positioning pre \
                     --target lm --tie_weights
```
GPT-2使用的语料的格式和GPT、RoBERTa一样。GPT-2的编码器不同于GPT的编码器，使用了前置layernorm（*--layernorm_positioning pre*），并在编码层的最后加入了一个额外的layernorm。此外，我们需要去掉接在embedding层后面的layernorm（*--remove_embedding_layernorm*）。


#### ELMo
ELMo预处理和预训练示例:
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --target bilm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --output_model_path models/output_model.bin \
                    --config_path models/birnn_config.json --learning_rate 5e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --embedding word --remove_embedding_layernorm --encoder bilstm --target bilm
```
ELMo使用的语料的格式和GPT-2一样。我们可以通过*--embedding word*，*--encoder bilstm* 和 *--target bilm*的组合来预训练ELMo。<br>
*--embedding word* 表示使用传统的词向量层，LSTM不需要位置向量。此外，我们加上 *--remove_embedding_layernorm* ，去掉了embedding层后面的layernorm。


#### T5
T5提出使用序列到序列的方式统一处理自然语言理解和自然语言生成任务。T5充分探索了预训练相关的技术并进行了系统的对比，推荐使用编码器-解码器结构以及BERT-style预训练目标（模型对被遮罩的单词进行预测）。T5预处理和预训练示例:
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --seq_length 128 --dynamic_masking --target t5

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/t5/small_config.json \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --learning_rate 1e-3 --batch_size 64 \
                    --span_masking --span_geo_prob 0.3 --span_max_length 5 \
                    --embedding word --relative_position_embedding --remove_embedding_layernorm --tgt_embedding word \
                    --encoder transformer --mask fully_visible --layernorm_positioning pre --decoder transformer \
                    --target t5 --tie_weights
```
T5使用的语料的格式和GPT-2一样。我们通过 *--relative_position_embedding* 指定使用相对位置编码；通过 *--remove_embedding_layernorm* 和 *--layernorm_positioning pre* 指定使用前置layernorm（和GPT-2一样）。由于T5使用编码器-解码器结构，因此我们需要同时指定 *--encoder* 和 *--decoder* 。


#### T5-v1_1
T5-v1_1在T5的基础上进行了多项的改进。T5-v1_1预处理和预训练示例:
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --processes_num 8 --seq_length 128 --dynamic_masking --target t5

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --config_path models/t5-v1_1/small_config.json \
                    --output_model_path models/output_model.bin \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \
                    --learning_rate 1e-3 --batch_size 64 \
                    --span_masking --span_geo_prob 0.3 --span_max_length 5 \
                    --embedding word --relative_position_embedding --remove_embedding_layernorm --tgt_embedding word \
                    --encoder transformer --mask fully_visible --layernorm_positioning pre --feed_forward gated --decoder transformer \
                    --target t5
```
T5-v1_1使用的语料的格式和T5一样。通过 *--feed_forward* 指定feed-forward层的类型。*--tie_weights* 被去掉，这样embedding层和softmax前一层不共享参数。此外，T5-v1_1和T5有着不同的配置文件。 


#### 更多组合
使用LSTM编码器和语言模型目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --output_model_path models/output_model.bin \
                    --config_path models/rnn_config.json --learning_rate 1e-3 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --total_steps 20000 --save_checkpoint_steps 5000 \
                    --embedding word --remove_embedding_layernorm --encoder lstm --target lm
```
我们使用*models/rnn_config.json*作为配置文件。

使用GRU编码器和语言模型目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --output_model_path models/output_model.bin \
                    --config_path models/rnn_config.json --learning_rate 1e-3 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --total_steps 20000 --save_checkpoint_steps 5000 \
                    --embedding word --remove_embedding_layernorm --encoder gru --target lm
```

使用GatedCNN编码器和语言模型目标任务预训练示例：
```
python3 preprocess.py --corpus_path corpora/book_review.txt --vocab_path models/google_zh_vocab.txt --dataset_path dataset.pt --processes_num 8 --target lm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --output_model_path models/output_model.bin \
                    --config_path models/gatedcnn_9_config.json --learning_rate 1e-4 \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --total_steps 20000 --save_checkpoint_steps 5000 \
                    --embedding word --remove_embedding_layernorm --encoder gatedcnn --target lm
```

使用机器翻译语料和任务进行预训练示例，目标任务和CoVe一样，但是使用Transformer作为编码器和解码器：
```
python3 preprocess.py --corpus_path corpora/iwslt_15_zh_en.tsv \
                      --vocab_path models/google_zh_vocab.txt --tgt_vocab_path models/google_uncased_en_vocab.txt \
                      --dataset_path dataset.pt --seq_length 64 --tgt_seq_length 64 --processes_num 8 --target seq2seq

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt --tgt_vocab_path models/google_uncased_en_vocab.txt \
                    --output_model_path output_model.bin --config_path models/encoder_decoder_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --report_steps 1000 --total_steps 50000 --save_checkpoint_steps 10000 \
                    --embedding word_sinusoidalpos --tgt_embedding word_sinusoidalpos \
                    --encoder transformer --mask fully_visible --decoder transformer \
                    --target seq2seq
```
其中 [*iwslt_15_zh_en.tsv*](https://share.weiyun.com/hIKCDpf6) 为中英平行语料，源语言和目标语言用\t分隔，这也是 *--target seq2seq* 需要的语料格式。我们可以使用预训练模型的编码器部分去初始化下游任务模型。

使用Prefix LM（在UniLM中被使用）作为目标任务进行预训练示例：
```
python3 preprocess.py --corpus_path corpora/csl_title_abstract.txt --vocab_path models/google_zh_vocab.txt \
                      --dataset_path dataset.pt --seq_length 256 --processes_num 8 --target prefixlm

python3 pretrain.py --dataset_path dataset.pt --vocab_path models/google_zh_vocab.txt \
                    --output_model_path output_model.bin --config_path models/bert/base_config.json \
                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 --learning_rate 1e-4 \
                    --total_steps 5000 --save_checkpoint_steps 100 \
                    --embedding word_pos_seg --encoder transformer --mask causal_with_prefix --target prefixlm
```
其中 [*csl_title_abstract.txt*](https://share.weiyun.com/LwuQwWVl) 为科学文献标题摘要语料，标题和摘要用\t分隔，这也是 *--target prefixlm* 需要的语料格式。我们通过 *--mask causal_with_prefix* 指定前缀信息使用fully_visible方式遮罩，其余信息使用causal方式遮罩。需要注意的是，模型需要根据segment信息区分哪些部分是前缀。因此必须选择 *--embedding word_pos_seg* 。
<br>
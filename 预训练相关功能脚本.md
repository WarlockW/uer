UER-py 为预训练模型提供了丰富的脚本。这里首先列举项目包括的脚本以及它们的功能，然后详细介绍几个脚本的使用方式。
<table>
<tr align="center"><th> 脚本名 <th> 功能描述
<tr align="center"><td> average_model.py <td> 对多个模型的参数取平均，比如对不同训练步数的模型参数取平均，增加模型的鲁棒性
<tr align="center"><td> build_vocab.py <td> 根据给定的语料和分词器构造词典
<tr align="center"><td> check_model.py <td> 查看模型是多GPU版本，还是单GPU版本，测试加载单GPU版本模型是否成功
<tr align="center"><td> cloze_test.py <td> 随机遮住单词并进行预测，返回前n个预测结果
<tr align="center"><td> convert_bert_from_uer_to_google.py <td> 将本项目的预训练BERT模型转到Google官方的格式
<tr align="center"><td> convert_bert_from_uer_to_huggingface.py <td> 将本项目的预训练BERT模型转到Huggingface的格式
<tr align="center"><td> convert_bert_from_google_to_uer.py <td> 将Google官方预训练BERT模型的格式转到本项目的格式
<tr align="center"><td> convert_bert_from_huggingface_to_uer.py <td> 将Huggingface预训练BERT模型的格式转到本项目的格式
<tr align="center"><td> convert_albert_from_brightmart_to_uer.py <td> 将brightmart的预训练AlBERT模型转到本项目的格式
<tr align="center"><td> diff_vocab.py <td> 比较两个词典的重合度
<tr align="center"><td> dynamic_vocab_adapter.py <td> 根据词典调整模型，使模型和词典配套
<tr align="center"><td> extract_embeddings.py <td> 抽取预训练模型的embedding层
<tr align="center"><td> extract_features.py <td> 抽取预训练模型的最后一层隐层表示
<tr align="center"><td> topn_words_dep.py <td> 上下文相关的以词搜词，根据最后一层的隐层表示进行最近邻检索，详细的检索原理在下面进行介绍
<tr align="center"><td> topn_words_indep.py <td> 上下文无关的以词搜词，根据embedding层词向量进行最近邻检索
</table>

#### 完形填空
*cloze_test.py* 基于MLM任务，对遮住的词进行预测，返回topn最有可能的词。可以在cloze_test.py的基础上进行数据增强等操作。*cloze_test.py* 使用示例：
```
python3 scripts/cloze_test.py --load_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                              --config_path models/bert/base_config.json \
                              --test_path datasets/tencent_profile.txt --prediction_path output.txt \
                              --target bert
```
注意到完形填空的预训练目标 *--target* 只能选择包括MLM的类型。更具体的，*cloze_test.py* 支持bert，mlm，和albert预训练目标。

#### 特征抽取
*extract_features.py* 让文本经过词向量层，编码层，pooling层，得到文本表示。*extract_features.py* 使用示例：
```
python3 scripts/extract_features.py --load_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                    --config_path models/bert/base_config.json \
                                    --test_path datasets/tencent_profile.txt --prediction_path features.pt \
                                    --pooling first
```
文本经过BERT模型的词向量层以及编码层，再取第一个位置，也就是[CLS]位置的向量（*--pooling first*），作为文本表示。但是当我们使用余弦衡量文本相似度的时候，上面这种文本表示方式效果不好。根据最新的研究工作，我们对模型生成的文本表示进行白化操作：
```
python3 scripts/extract_features.py --load_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                    --config_path models/bert/base_config.json \
                                    --test_path datasets/tencent_profile.txt --prediction_path features.pt \
                                    --pooling first --whitening_size 64
```
*--whitening_size 64* 表明会使用白化操作，并且向量经过变化后，维度变为64。如果不指定 *--whitening_size* ，则不会使用白化操作。推荐在特征抽取过程中使用白化操作。

#### 词向量抽取
*extract_embeddings.py* 从预训练模型权重embedding层中抽取词向量。这里的词向量指传统的上下文无关词向量。抽取出的词向量可以用于初始化其他模型（比如CNN）词向量层初始化。*extract_embeddings.py* 使用示例：
```
python3 scripts/extract_embeddings.py --load_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                      --word_embedding_path embeddings.txt
```
*--word_embedding_path* 指定输出词向量文件的路径。词向量文件的格式遵循[这里](https://github.com/Embedding/Chinese-Word-Vectors)，可以被主流项目直接使用。

#### 以词搜词
预训练模型能够产生高质量的词向量。传统的词向量（比如word2vec和GloVe）给定一个单词固定的向量（上下文无关向量）。然而，一词多义是人类语言中的常见现象。一个单词的意思依赖于其上下文。我们可以使用预训练模型的隐层去表示单词。值得注意的是大多数的中文预训练模型是基于字的。如果需要真正的词向量而不是字向量，用户需要下载[基于词的BERT模型](https://share.weiyun.com/5s4HVMi)和[词典](https://share.weiyun.com/5NWYbYn)。下面给出上下文无关词向量以词搜词scripts/topn_words_indep.py示例（基于字和基于词）：
```
python3 scripts/topn_words_indep.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                    --cand_vocab_path models/google_zh_vocab.txt --target_words_path target_words.txt
python3 scripts/topn_words_indep.py --pretrained_model_path models/bert_wiki_word_model.bin --vocab_path models/wiki_word_vocab.txt \
                                    --cand_vocab_path models/wiki_word_vocab.txt --target_words_path target_words.txt
```
上下文无关词向量来自于模型的embedding层，target_words.txt的格式如下所示：
```
word-1
word-2
...
word-n
```
下面给出上下文相关词向量以词搜词scripts/topn_words_dep.py示例（基于字和基于词）：
```
python3 scripts/topn_words_dep.py --pretrained_model_path models/google_zh_model.bin --vocab_path models/google_zh_vocab.txt \
                                  --cand_vocab_path models/google_zh_vocab.txt --sent_path target_words_with_sentences.txt --config_path models/bert_base_config.json \
                                  --batch_size 256 --seq_length 32 --tokenizer bert
python3 scripts/topn_words_dep.py --pretrained_model_path models/bert_wiki_word_model.bin --vocab_path models/wiki_word_vocab.txt \
                                  --cand_vocab_path models/wiki_word_vocab.txt --sent_path target_words_with_sentences.txt --config_path models/bert_base_config.json \
                                  --batch_size 256 --seq_length 32 --tokenizer space
```
我们把目标词替换成词典中其它的词，将序列送入网络。网络的隐层可以看作是上下文相关的词向量。注意到如果用户用基于词的模型，需要对target_words_with_sentences.txt文件的句子进行分词。这个文件的格式如下：
```
sent1 word1
sent1 word1
...
sentn wordn
```
句子与单词之间使用\t分割

#### 文本生成
我们可以使用 *generate.py* 来生成文本，给定一些词句，使用*generate.py* 可以进行续写。
 *generate.py* 使用示例：
```
python3 scripts/generate.py --pretrained_model_path models/gpt_model.bin --vocab_path models/google_zh_vocab.txt 
                            --input_path story_beginning.txt --output_path story_full.txt --config_path models/bert_base_config.json 
                            --encoder gpt --target lm --seq_length 128  
```
*story_beginning* 包含了文本的开头，*--pretrained_model_path* 可以使用经过LM目标预先训练的任何模型，例如 [在混合大型语料库上训练的GPT](https://share.weiyun.com/51nTP8V)。到目前为止，我们仅提供普通版本的文本生成器，我们之后将添加更多机制以提高性能和效率。

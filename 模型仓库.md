[**English**](https://github.com/dbiir/UER-py/wiki/Modelzoo) | [**中文**](https://github.com/dbiir/UER-py/wiki/模型仓库)

借助UER，我们使用不同的语料、编码器和目标任务对模型进行了预训练，所有预训练模型都可以由UER直接加载。未来会有更多预训练模型发布。除非另有说明，否则中文预训练模型将使用 *models/google_zh_vocab.txt* 作为词典，该词典在原始BERT项目中使用； *models/bert_base_config.json* 用作默认配置文件；常用的词汇和配置文件包含在 *models* 文件夹中，用户无需下载。

来自Google的中文预训练模型（采用UER格式）：
<table>
<tr align="center"><th> 预训练模型 <th> Link <th> 描述 
<tr align="center"><td> Wikizh+BertEncoder+BertTarget <td> https://share.weiyun.com/A1C49VPb <td> 来自https://github.com/google-research/bert的谷歌中文预训练模型
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(base)+AlbertTarget <td> https://share.weiyun.com/UnKHNKRG <td> 来自https://github.com/google-research/albert谷歌中文预训练模型。<br>配置文件为albert_base_config.json
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(large)+AlbertTarget <td> https://share.weiyun.com/9tTUwALd <td> 来自https://github.com/google-research/albert的谷歌中文预训练模型。 <br>配置文件为albert_large_config.json
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(xlarge)+AlbertTarget <td> https://share.weiyun.com/mUamRQFR <td> 来自https://github.com/google-research/albert的谷歌中文预训练模型。 <br>配置文件为albert_xlarge_config.json
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(xxlarge)+AlbertTarget <td> https://share.weiyun.com/0i2lX62b <td> 来自https://github.com/google-research/albert的谷歌中文预训练模型。 <br>配置文件为albert_xxlarge_config.json
</table>

UER预训练模型
<table>
<tr align="center"><th> 预训练模型 <th> Link <th> 描述 
<tr align="center"><td> Wikizh(word-based)+BertEncoder+BertTarget <td> 模型：https://share.weiyun.com/5s4HVMi 词典：https://share.weiyun.com/5NWYbYn <td> 在中文维基百科语料上训练的基于词的BERT模型，训练步数：50万步。
<tr align="center"><td> RenMinRiBao+BertEncoder+BertTarget <td> https://share.weiyun.com/5JWVjSE <td> 训练语料为人民日报，适合用于新闻相关的数据集。
<tr align="center"><td> Webqa2019+BertEncoder+BertTarget <td> https://share.weiyun.com/5HYbmBh <td> 训练语料为社区问答，包含问题，问题描述，答案，适合社交场景相关的数据集，训练步数：50万步。
<tr align="center"><td> Weibo+BertEncoder+BertTarget <td> https://share.weiyun.com/5ZDZi4A <td> 训练语料为微博。
<tr align="center"><td> Weibo+BertEncoder(large)+MlmTarget <td> https://share.weiyun.com/CFKyMkp3 <td> 训练语料为微博。配置文件为bert_large_config.json
<tr align="center"><td> Reviews+BertEncoder+MlmTarget <td> https://share.weiyun.com/tBgaSx77 <td> 训练语料为评论。
<tr align="center"><td> Reviews+BertEncoder(large)+MlmTarget <td> https://share.weiyun.com/hn7kp9bs <td> 训练语料为评论，配置文件为 bert_large_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(large)+BertTarget <td> https://share.weiyun.com/5G90sMJ <td> 训练语料为大规模混合语料，配置文件是bert_large_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(base)+BertTarget <td> https://share.weiyun.com/5QOzPqq <td> 训练语料为大规模混合语料，配置文件是bert_base_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(small)+BertTarget <td> https://share.weiyun.com/fhcUanfy <td> 训练语料为大规模混合语料，配置文件是bert_small_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(tiny)+BertTarget <td> https://share.weiyun.com/yXx0lfUg <td> 训练语料为大规模混合语料，配置文件是bert_tiny_config.json
<tr align="center"><td> MixedCorpus+GptEncoder+LmTarget <td> https://share.weiyun.com/51nTP8V <td> 训练语料为大规模混合语料，训练步数为50万 (句长128) + 10万 (句长512)
<tr align="center"><td> Reviews+LstmEncoder+LmTarget <td> https://share.weiyun.com/57dZhqo  <td> 训练语料是亚马逊网站评论 + 京东评论 + 点评 (共11.4M 评论)，使用语言模型作为目标任务。适用于与评论相关的数据集。与随机初始化相比，它在某些评论数据集上实现了5％以上的改进。在使用它之前，将models/rnn_config.json中的hidden_size设置为512。训练步数：200,000；句长：128。
<tr align="center"><td> (MixedCorpus & Amazon reviews)+LstmEncoder+(LmTarget & ClsTarget) <td> https://share.weiyun.com/5B671Ik  <td> 首先以LM作为目标任务对混合中文大语料库进行预训练，然后使用lm目标任务和cls目标任务对亚马逊评论进行预训练。它适用于与评论相关的数据集。在某些评论数据集上，它可以与BERT取得可比的结果。 训练步数：500,000 + 100,000； 句长：128
<tr align="center"><td> IfengNews+BertEncoder+BertTarget <td> https://share.weiyun.com/5HVcUWO <td> 训练语料是来自Ifeng网站的新闻数据，使用新闻标题来预测新闻摘要。训练步数：100,000； 句长：128
<tr align="center"><td> jdbinary+BertEncoder+ClsTarget <td> https://share.weiyun.com/596k2bu <td> 训练语料是京东的审查数据，以cls作为目标任务进行预训练。它适用于与购物评论相关的数据集。训练步数：50,000；句长：128
<tr align="center"><td> jdfull+BertEncoder+MlmTarget <td> https://share.weiyun.com/5L6EkUF <td> 训练语料是京东的审查数据，以mlm作为目标任务进行预训练。训练步数：50,000；句长：128
<tr align="center"><td> Amazonreview+BertEncoder+ClsTarget <td> https://share.weiyun.com/5XuxtFA <td> 训练语料是来自亚马逊网站的评论数据（包括书评，电影评论等），以cls作为目标任务进行预训练。它适用于与评论相关的数据集，例如与Google BERT相比，该模型将豆瓣图书评论数据集的准确性从87.6提高到88.5。 训练步数：20,000； 句长：128
<tr align="center"><td> XNLI+BertEncoder+ClsTarget <td> https://share.weiyun.com/5oXPugA <td> 用BertEncoder进行推理
</table>
MixedCorpus包含百度百科，中文维基百科，网站问答，人民日报，文献和评论。
<br/>
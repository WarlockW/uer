[**English**](https://github.com/dbiir/UER-py/wiki/Modelzoo) | [**中文**](https://github.com/dbiir/UER-py/wiki/模型仓库)

借助UER，我们使用不同的语料、编码器和目标任务对模型进行了预训练。所有预训练模型都可以由UER直接加载。未来会发布更多的预训练模型。除非另有说明，否则中文预训练模型将使用 *models/google_zh_vocab.txt* 作为词典（原始Google BERT项目中的中文词典）以及BERT tokenizer作为分词器。*models/bert/base_config.json* 为默认的配置文件；常用的词典和配置文件包含在 *models* 文件夹中，用户无需下载。此外，我们通过 *scripts/convert_xxx_from_uer_to_huggingface.py* 将UER预训练的模型转换为Huggingface Transformers支持的格式，并上传到了[Huggingface模型仓库](https://huggingface.co/uer)。受限于篇幅，本章发布的预训练权重的细节描述，也放到了相应的Huggingface模型仓库上。用户可以一一对应上。

## 中文RoBERTa预训练模型
24个不同尺寸的中文RoBERTa预训练模型。语料为CLUECorpusSmall。配置文件在 *models/bert/* 路径下。我们只为Tiny，Mini，Small，Medium，Base，Large模型提供了配置文件。为了加载下面的其他模型，我们需要修改配置文件中的 *emb_size*，*feedforward_size*，*hidden_size*，*heads_num*，*layers_num*。注意到*emb_size*等于*hidden_size*，*feedforward_size*是*hidden_size*的4倍，*heads_num*等于*hidden_size*除以64。更多的细节请参考[这里](https://huggingface.co/uer/chinese_roberta_L-2_H-128)。

下面列出不同层数（*layers_num*），不同隐层维度（*hidden_size*）的中文RoBERTa预训练权重链接：

|层数/隐层维度|           H=128           |           H=256           |            H=512            |            H=768            |
| -------- | :-----------------------: | :-----------------------: | :-------------------------: | :-------------------------: |
| **L=2**  | [**2/128 (Tiny)**][2_128] |      [2/256][2_256]       |       [2/512][2_512]        |       [2/768][2_768]        |
| **L=4**  |      [4/128][4_128]       | [**4/256 (Mini)**][4_256] | [**4/512 (Small)**][4_512]  |       [4/768][4_768]        |
| **L=6**  |      [6/128][6_128]       |      [6/256][6_256]       |       [6/512][6_512]        |       [6/768][6_768]        |
| **L=8**  |      [8/128][8_128]       |      [8/256][8_256]       | [**8/512 (Medium)**][8_512] |       [8/768][8_768]        |
| **L=10** |     [10/128][10_128]      |     [10/256][10_256]      |      [10/512][10_512]       |      [10/768][10_768]       |
| **L=12** |     [12/128][12_128]      |     [12/256][12_256]      |      [12/512][12_512]       | [**12/768 (Base)**][12_768] |


## 基于词的中文RoBERTa预训练模型
5个不同尺寸的基于词的中文RoBERTa预训练模型。语料为CLUECorpusSmall。分词工具为Google sentencepiece，使用的sentencepiece模型为 *--spm_model_path models/cluecorpussmall_spm.model* 。配置文件在 *models/bert/* 路径下。目前主流的中文预训练模型是基于字的。我们发现基于词的预训练模型在下游任务上往往有更好的效果，并且在推理速度上更有优势。更多的细节请参考[这里](https://huggingface.co/uer/roberta-tiny-word-chinese-cluecorpussmall)。

下面列出不同尺寸的基于词的中文RoBERTa预训练权重链接：

|           模型链接           |
| :-----------------------:|
| [**L=2/H=128 (Tiny)**][word_tiny] |
| [**L=4/H=256 (Mini)**][word_mini] |
| [**L=4/H=512 (Small)**][word_small] |
| [**L=8/H=512 (Medium)**][word_medium] |
| [**L=12/H=768 (Base)**][word_base] |

## 基于不同语料的中文GPT-2预训练模型
我们基于不同的语料，训练了一系列GPT-2语言模型。配置文件在 *models/gpt2/* 路径下。下面列出它们的权重链接和细节描述链接（Huggingface模型仓库）：

|           模型链接           |           细节描述链接           |
| :-----------------------:| :-----------------------:|
| [**通用中文GPT-2预训练模型**][gpt2_cluecorpussmall] | https://huggingface.co/uer/gpt2-chinese-cluecorpussmall |
| [**通用中文GPT-2预训练小模型**][gpt2_distil_cluecorpussmall] | https://huggingface.co/uer/gpt2-distil-chinese-cluecorpussmall |
| [**古诗词GPT-2预训练模型**][gpt2_poem] | https://huggingface.co/uer/gpt2-chinese-poem |
| [**对联GPT-2预训练模型**][gpt2_couplet] | https://huggingface.co/uer/gpt2-chinese-couplet |
| [**中文歌词GPT-2预训练模型**][gpt2_lyric] | https://huggingface.co/uer/gpt2-chinese-lyric |
| [**文言文GPT-2预训练模型**][gpt2_ancient] | https://huggingface.co/uer/gpt2-chinese-ancient |

## 其他预训练模型
来自Google的中文预训练模型（采用UER格式）：
<table>
<tr align="center"><th> 预训练模型 <th> Link <th> 描述 
<tr align="center"><td> Wikizh+BertEncoder+BertTarget <td> https://share.weiyun.com/A1C49VPb <td> 来自https://github.com/google-research/bert的谷歌中文预训练模型
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(base)+AlbertTarget <td> https://share.weiyun.com/UnKHNKRG <td> 来自https://github.com/google-research/albert谷歌中文预训练模型。<br>配置文件为albert_base_config.json
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(large)+AlbertTarget <td> https://share.weiyun.com/9tTUwALd <td> 来自https://github.com/google-research/albert的谷歌中文预训练模型。 <br>配置文件为albert_large_config.json
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(xlarge)+AlbertTarget <td> https://share.weiyun.com/mUamRQFR <td> 来自https://github.com/google-research/albert的谷歌中文预训练模型。 <br>配置文件为albert_xlarge_config.json
<tr align="center"><td> CLUECorpus+<br>AlbertEncoder(xxlarge)+AlbertTarget <td> https://share.weiyun.com/0i2lX62b <td> 来自https://github.com/google-research/albert的谷歌中文预训练模型。 <br>配置文件为albert_xxlarge_config.json
</table>

UER预训练模型
<table>
<tr align="center"><th> 预训练模型 <th> Link <th> 描述 
<tr align="center"><td> Wikizh(word-based)+BertEncoder+BertTarget <td> 模型：https://share.weiyun.com/5s4HVMi 词典：https://share.weiyun.com/5NWYbYn <td> 在中文维基百科语料上训练的基于词的BERT模型，训练步数：50万步。
<tr align="center"><td> RenMinRiBao+BertEncoder+BertTarget <td> https://share.weiyun.com/5JWVjSE <td> 训练语料为人民日报，适合用于新闻相关的数据集。
<tr align="center"><td> Webqa2019+BertEncoder+BertTarget <td> https://share.weiyun.com/5HYbmBh <td> 训练语料为社区问答，包含问题，问题描述，答案，适合社交场景相关的数据集，训练步数：50万步。
<tr align="center"><td> Weibo+BertEncoder+BertTarget <td> https://share.weiyun.com/5ZDZi4A <td> 训练语料为微博。
<tr align="center"><td> Weibo+BertEncoder(large)+MlmTarget <td> https://share.weiyun.com/CFKyMkp3 <td> 训练语料为微博。配置文件为bert_large_config.json
<tr align="center"><td> Reviews+BertEncoder+MlmTarget <td> https://share.weiyun.com/tBgaSx77 <td> 训练语料为评论。
<tr align="center"><td> Reviews+BertEncoder(large)+MlmTarget <td> https://share.weiyun.com/hn7kp9bs <td> 训练语料为评论，配置文件为 bert_large_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(large)+BertTarget <td> https://share.weiyun.com/5G90sMJ <td> 训练语料为大规模混合语料，配置文件是bert_large_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(base)+BertTarget <td> https://share.weiyun.com/5QOzPqq <td> 训练语料为大规模混合语料，配置文件是bert_base_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(small)+BertTarget <td> https://share.weiyun.com/fhcUanfy <td> 训练语料为大规模混合语料，配置文件是bert_small_config.json
<tr align="center"><td> MixedCorpus+BertEncoder(tiny)+BertTarget <td> https://share.weiyun.com/yXx0lfUg <td> 训练语料为大规模混合语料，配置文件是bert_tiny_config.json
<tr align="center"><td> MixedCorpus+GptEncoder+LmTarget <td> https://share.weiyun.com/51nTP8V <td> 训练语料为大规模混合语料，训练步数为50万 (句长128) + 10万 (句长512)
<tr align="center"><td> Reviews+LstmEncoder+LmTarget <td> https://share.weiyun.com/57dZhqo  <td> 训练语料是亚马逊网站评论 + 京东评论 + 点评 (共11.4M 评论)，使用语言模型作为目标任务。适用于与评论相关的数据集。与随机初始化相比，它在某些评论数据集上实现了5％以上的改进。在使用它之前，将models/rnn_config.json中的hidden_size设置为512。训练步数：200,000；句长：128。
<tr align="center"><td> (MixedCorpus & Amazon reviews)+LstmEncoder+(LmTarget & ClsTarget) <td> https://share.weiyun.com/5B671Ik  <td> 首先以LM作为目标任务对混合中文大语料库进行预训练，然后使用lm目标任务和cls目标任务对亚马逊评论进行预训练。它适用于与评论相关的数据集。在某些评论数据集上，它可以与BERT取得可比的结果。 训练步数：500,000 + 100,000； 句长：128
<tr align="center"><td> IfengNews+BertEncoder+BertTarget <td> https://share.weiyun.com/5HVcUWO <td> 训练语料是来自Ifeng网站的新闻数据，使用新闻标题来预测新闻摘要。训练步数：100,000； 句长：128
<tr align="center"><td> jdbinary+BertEncoder+ClsTarget <td> https://share.weiyun.com/596k2bu <td> 训练语料是京东的审查数据，以cls作为目标任务进行预训练。它适用于与购物评论相关的数据集。训练步数：50,000；句长：128
<tr align="center"><td> jdfull+BertEncoder+MlmTarget <td> https://share.weiyun.com/5L6EkUF <td> 训练语料是京东的审查数据，以mlm作为目标任务进行预训练。训练步数：50,000；句长：128
<tr align="center"><td> Amazonreview+BertEncoder+ClsTarget <td> https://share.weiyun.com/5XuxtFA <td> 训练语料是来自亚马逊网站的评论数据（包括书评，电影评论等），以cls作为目标任务进行预训练。它适用于与评论相关的数据集，例如与Google BERT相比，该模型将豆瓣图书评论数据集的准确性从87.6提高到88.5。 训练步数：20,000； 句长：128
<tr align="center"><td> XNLI+BertEncoder+ClsTarget <td> https://share.weiyun.com/5oXPugA <td> 用BertEncoder进行推理
</table>
MixedCorpus包含百度百科，中文维基百科，网站问答，人民日报，文献和评论。
<br/>

[2_128]:https://share.weiyun.com/5mXkEvxN
[2_256]:https://share.weiyun.com/TLn5VpSW
[2_512]:https://share.weiyun.com/nYtFjDBL
[2_768]:https://share.weiyun.com/sHMGMv4c
[4_128]:https://share.weiyun.com/0SlDbnSM
[4_256]:https://share.weiyun.com/0eNSFKw9
[4_512]:https://share.weiyun.com/SEJ0MStj
[4_768]:https://share.weiyun.com/jk42uruV  
[6_128]:https://share.weiyun.com/tWeXueMJ
[6_256]:https://share.weiyun.com/1Ku03xFM
[6_512]:https://share.weiyun.com/WEvmJUOD
[6_768]:https://share.weiyun.com/m5AMthBU
[8_128]:https://share.weiyun.com/r0M1pJsV
[8_256]:https://share.weiyun.com/xk1SN8V9
[8_512]:https://share.weiyun.com/vpnaZ5jp
[8_768]:https://share.weiyun.com/NT67VIap
[10_128]:https://share.weiyun.com/dPKrelIL
[10_256]:https://share.weiyun.com/dbn2bG0t
[10_512]:https://share.weiyun.com/q8yIZWje
[10_768]:https://share.weiyun.com/dz4sgiCx
[12_128]:https://share.weiyun.com/93UYnnSC
[12_256]:https://share.weiyun.com/czAR5KNu
[12_512]:https://share.weiyun.com/gv4zARxk
[12_768]:https://share.weiyun.com/2rEWrSQz

[word_tiny]:https://share.weiyun.com/6mUaN18A
[word_mini]:https://share.weiyun.com/og1Km7qM
[word_small]:https://share.weiyun.com/SqbCfIgp
[word_medium]:https://share.weiyun.com/yDz44dlS
[word_base]:https://share.weiyun.com/5OXC8Rzt

[gpt2_cluecorpussmall]:https://share.weiyun.com/0eAlQWRB
[gpt2_distil_cluecorpussmall]:https://share.weiyun.com/IAvDbjKR
[gpt2_poem]:https://share.weiyun.com/DKAmuOLU
[gpt2_couplet]:https://share.weiyun.com/LbMecOGj
[gpt2_lyric]:https://share.weiyun.com/jBv5weES
[gpt2_ancient]:https://share.weiyun.com/d3FHbVMx